[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Quarto CRC Book",
    "section": "",
    "text": "Preface\nThis is a Quarto book."
  },
  {
    "objectID": "index.html#software-conventions",
    "href": "index.html#software-conventions",
    "title": "Quarto CRC Book",
    "section": "Software conventions",
    "text": "Software conventions\n\n1 + 1\n\n2\n\n\nTo learn more about Quarto books visit https://quarto.org/docs/books."
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Quarto CRC Book",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nBlah, blah, blah…"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "",
    "text": "2 DATA UNDERSTANDING\nData Understanding adalah tahap yang fokus pada eksplorasi dan pemahaman awal terhadap data yang akan digunakan dalam suatu proyek analisis data. Tahap ini merupakan langkah untuk memahami karakteristik, struktur, dan konten atau isi dari data sebelum melakukan analisis lebih lanjut. data understanding dipakai untuk memeriksa data sehingga dapat mengidentifikasi masalah pada data yang kita dapatkan.\nPreprocessing (pra-pemrosesan) adalah tahap dalam analisis data yang bertujuan untuk membersihkan, mengorganisir, dan menyesuaikan data sehingga dapat digunakan secara efektif untuk pemodelan atau analisis lebih lanjut.\nEvaluasi dalam konteks model prediktif dan klasifikasi merujuk pada penilaian kinerja model terhadap data yang belum pernah dilihat sebelumnya atau data uji. Tujuan evaluasi adalah untuk memahami sejauh mana model dapat menggeneralis ke data baru dan seberapa baik model dapat melakukan tugasnya, seperti klasifikasi dengan akurasi tinggi atau regresi dengan presisi yang baik. Berikut adalah beberapa metrik evaluasi umum:\nSilahkan klik Disini untuk melihat deploynya."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#determine-business-objectives-menentukan-tujuan-bisnis",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#determine-business-objectives-menentukan-tujuan-bisnis",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "1.1 Determine Business Objectives (Menentukan Tujuan Bisnis)",
    "text": "1.1 Determine Business Objectives (Menentukan Tujuan Bisnis)\nAir merupakan salah satu sumber daya alam esensial untuk kelangsungan hidup seluruh makhluk hidup di dunia ini. Kita memerlukan air untuk kebutuhan kita sehari-hari, begitu juga dengan tanaman dan hewan yang memerlukan air untuk kelangsungan hidupnya. Menurut data dari World Health Organization (WHO), diperkirakan sebanyak 1.1 Miliar orang tidak memiliki akses untuk mendapatkan air yang layak minum dan 2.6 Miliar lainnya tidak mendapatkan fasilitas sanitasi yang baik. Pengolahan data ini bertujuan untuk menilai dan mengategorikan kualitas air sebagai “aman” atau “tidak aman” menggunakan teknik klasifikasi. Ini sejalan dengan tujuan lebih besar untuk memastikan air minum yang aman bagi masyarakat. Dalam pengolahan data kali ini akan dilakukan pencarian terhadap metode klasifikasi dan Apa fitur utama yang paling signifikan berkontribusi pada klasifikasi air sebagai aman atau tidak aman?"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#access-situation-menilai-situasi",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#access-situation-menilai-situasi",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "1.2 Access Situation (Menilai Situasi)",
    "text": "1.2 Access Situation (Menilai Situasi)\nSetelah memahami dengan lebih spesifik tujuan bisnis terkait penilaian kualitas air, langkah berikutnya adalah melakukan assessment yang mencakup ketersediaan sumber daya, mitigasi risiko, dan terutama ketersediaan data kualitas air. Data mengenai kualitas air dapat bervariasi tergantung pada sumbernya, dan metode analitika data yang akan diterapkan akan sangat dipengaruhi oleh ketersediaan data tersebut. Dalam konteks kualitas air, pertanyaan yang muncul adalah, “Apa fitur utama yang paling signifikan berkontribusi pada klasifikasi air sebagai aman atau tidak aman?” Untuk menjawab pertanyaan ini, metode klasifikasi atau estimasi dapat diterapkan. Misalnya, algoritma machine learning seperti Random Forest atau Logistic Regression dapat digunakan untuk mengidentifikasi faktor-faktor utama yang membedakan air aman dan tidak aman."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#determine-data-mining-goals-menentukan-tujuan-data-mining",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#determine-data-mining-goals-menentukan-tujuan-data-mining",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "1.3 Determine Data Mining Goals (Menentukan Tujuan Data Mining)",
    "text": "1.3 Determine Data Mining Goals (Menentukan Tujuan Data Mining)\nTujuan Data Mining kali ini yakni untuk mengembangkan model klasifikasi yang dapat mengidentifikasi apakah suatu sampel air aman atau tidak aman berdasarkan parameter-parameter tertentu. Air minum yang layak konsumsi harus memenuhi beberapa kriteria, seperti tidak berwarna, tidak berbau, dan tidak mengandung zat berbahaya. Kriteria ini penting untuk diketahui agar terhindar dari masalah kesehatan akibat konsumsi air minum yang tidak layak. dalam menilai suatu air dikategorikan layak atau tidak layak di konsumsi jika berdasarkan kandungan zat yang ada di dalam air, maka berikut ini beberapa ciri atau kandungan air yang mempengaruhi kualitas air: 1. aluminium berbahaya jika lebih besar dari 2,8 2. ammonia berbahaya jika lebih besar dari 32,5 3. arsenic berbahaya jika lebih besar dari 0,01 4. barium berbahaya jika lebih besar dari 2 5. cadmium berbahaya jika lebih besar dari 0,005 6. chloramine berbahaya jika lebih besar dari 4 7. chromium berbahaya jika lebih besar dari 0,1 8. copper berbahaya jika lebih besar dari 1,3 9. flouride berbahaya jika lebih besar dari 1,5 10. bacteria berbahaya jika lebih besar dari 0 11. viruses berbahaya jika lebih besar dari 0 12. lead berbahaya jika lebih besar dari 0,015 13. nitrates berbahaya jika lebih besar dari 10 14. nitrites berbahaya jika lebih besar dari 1 15. mercury erbahaya jika lebih besar dari 0,002 16. perchlorate berbahaya jika lebih besar dari 56 17. radium berbahaya jika lebih besar dari 5 18. selenium berbahaya jika lebih besar dari 0,5 19. silver berbahaya jika lebih besar dari 0,1 20. uranium berbahaya jika lebih besar dari 0,3\nKemudian berdasarkan ciri-ciri di atas, suatu air bisa dikategorikan menjadi kelas 1 (safe atau aman) dan 0 (not safe atau tidak aman)."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#produce-project-plan-menghasilkan-rencana-proyek",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#produce-project-plan-menghasilkan-rencana-proyek",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "1.4 Produce Project Plan (Menghasilkan Rencana Proyek)",
    "text": "1.4 Produce Project Plan (Menghasilkan Rencana Proyek)\nLangkah-langkah Proyek sebagai berikut :  - Menentukan bussiness understanding - Mengumpulkan dataset kualitas air yang mencakup informasi parameter-parameter yang sesuai. - Melakukan data understanding untuk memahami sebuah data sepert jumlahnya, daftar kolom, tipe data kolom, banayknya kelas, dan mengetahui data tersebut perlu atau tidak untuk dilakukan preprocessing. - Melakukan preprocessing terhadap dataset sebelum diterapkan teknik klasifikasi. - Membagi dataset menjadi set pelatihan (training set) dan set pengujian (testing set). - Menentukan teknik klasifikasi yang paling sesuai untuk proyek ini. - Mengembangkan, melatih, dan mengevaluasi model klasifikasi menggunakan set pelatihan. - Mengukur performa model menggunakan set pengujian. - Melakukan evaluasi terhadap model yang telah dikembangkan. - Mengimplementasikan model dalam aplikasi streamlit untuk mengetahui kualitas air.\n\nimport matplotlib.pyplot as plt\nimport IPython\nimport numpy as np\nimport scipy.stats\nimport seaborn as sns\nimport pandas as pd\nimport os\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nimport pickle\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.decomposition import PCA\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\n# from keras.models import Sequential\n# from keras.layers import Dense\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.neighbors import LocalOutlierFactor\nfrom imblearn.over_sampling import RandomOverSampler\nfrom sklearn.preprocessing import RobustScaler\nfrom collections import Counter\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nimport joblib\nfrom sklearn.preprocessing import RobustScaler"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-dataset",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-dataset",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.1 Deskripsi Dataset",
    "text": "2.1 Deskripsi Dataset\nDatasest waterquality merupakan kumpulan data yang dibuat dari data imajiner kualitas air di lingkungan perkotaan. Dataset ini mencakup data kadar mikroorganisme yang terkandung di dalam air seperti kadar aluminium, ammonia, arsenic, barium, cadmium, chloramine, chromium, copper, flouride, bacteria, viruses, lead, nitrates, nitrites, mercury, perchlorate, radium, selenium, silver, dan uranium. Data tersebut berisi 21 atribut dan 7999 record, record tersebut diberi label dengan variabel kelas is_safe, yang memungkinkan klasifikasi data menggunakan nilai 1 (safe atau aman) dan 0 (not_safe atau tidak aman) dikonsumsi. data ini saya dapatkan dari Kaggel dengan link berikut : https://www.kaggle.com/datasets/mssmartypants/water-quality/.\ndataset tersebut berisi “synthetic water quality data” atau “simulated water quality data” atau dengan kata lain sample dari data yang bersifat fiktif atau imajiner yang dibuat untuk keperluan pendidikan dan latihan.\n\n# Membaca data dari file csv\ndf = pd.read_csv('waterQuality1.csv')\ndf\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7994\n0.05\n7.78\n0.00\n1.95\n0.040\n0.10\n0.03\n0.03\n1.37\n0.00\n...\n0.197\n14.29\n1.00\n0.005\n3.57\n2.13\n0.09\n0.06\n0.03\n1\n\n\n7995\n0.05\n24.22\n0.02\n0.59\n0.010\n0.45\n0.02\n0.02\n1.48\n0.00\n...\n0.031\n10.27\n1.00\n0.001\n1.48\n1.11\n0.09\n0.10\n0.08\n1\n\n\n7996\n0.09\n6.85\n0.00\n0.61\n0.030\n0.05\n0.05\n0.02\n0.91\n0.00\n...\n0.182\n15.92\n1.00\n0.000\n1.35\n4.84\n0.00\n0.04\n0.05\n1\n\n\n7997\n0.01\n10\n0.01\n2.00\n0.000\n2.00\n0.00\n0.09\n0.00\n0.00\n...\n0.000\n0.00\n0.00\n0.000\n0.00\n0.00\n0.00\n0.00\n0.00\n1\n\n\n7998\n0.04\n6.85\n0.01\n0.70\n0.030\n0.05\n0.01\n0.03\n1.00\n0.00\n...\n0.182\n15.92\n1.00\n0.000\n1.35\n4.84\n0.00\n0.04\n0.05\n1\n\n\n\n\n7999 rows × 21 columns\n\n\n\nKode di atas digunakan untuk membuka file csv dan menampilkan visual isi dari dataset.\n\nprint(\"Banyaknya data : \", df.shape[0])\nprint(\"Banyaknya kolom : \", df.shape[1])\n\nBanyaknya data :  7999\nBanyaknya kolom :  21\n\n\nfungs shape sendiri digunakan untuk mengetahui dimensi dari dataframe atau ukuran baris dan kolomnya. df.shape[0]: Ini mencetak jumlah baris (data points) dalam DataFrame df. Dalam hal ini, jumlah barisnya adalah 7999. df.shape[1]: Ini mencetak jumlah kolom (fitur atau variabel) dalam DataFrame df. Dalam hal ini, jumlah kolomnya adalah 21. Dengan kata lain, DataFrame Anda memiliki 7999 baris dan 21 kolom."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-fitur-dataset",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-fitur-dataset",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.2 Deskripsi Fitur Dataset",
    "text": "2.2 Deskripsi Fitur Dataset\n\nAluminium : Merupakan kandungan aluminium dalam air. berbahaya jika lebih besar dari 2,8. kandungan aluminium yang berlebihan dapat menyebabkan masalah kesehatan, terutama pada sistem saraf.\nAmmonia : Kandungan ammonia (NH3) dalam air. Amonia adalah gas dengan bau yang tajam dan beracun dalam konsentrasi tinggi. berbahaya jika lebih besar dari 32,5. Kandungan ammonia yang tinggi dalam air dapat menyebabkan kerusakan organisme akuatik dan merusak kualitas air minum.\nArsenic : Kandungan arsenik dalam air. Arsenic adalah unsur kimia dalam tabel periodik dengan simbol As dan nomor atom 33. Arsenic dapat ditemukan secara alami di dalam kerak bumi dan digunakan dalam berbagai aplikasi industri, termasuk pembuatan kayu tahan air. arsenic berbahaya jika lebih besar dari 0,01. Kandungan arsenik yang tinggi dalam air minum dapat menyebabkan keracunan dan meningkatkan risiko kanker.\nBarium: Kandungan barium dalam air. Barium adalah unsur kimia dengan simbol Ba dan nomor atom 56. Barium digunakan dalam industri minyak dan gas, serta dalam radiografi medis. berbahaya jika lebih besar dari 2. Pemaparan jangka panjang terhadap barium dapat menyebabkan kerusakan organ dalam tubuh manusia.\nCadmium : Kandungan kadmium dalam air. Cadmium adalah unsur kimia dengan simbol Cd dan nomor atom 48. Cadmium digunakan dalam baterai, cat, dan plastik. berbahaya jika lebih besar dari 0,005. Pemaparan cadmium dapat menyebabkan masalah kesehatan serius, termasuk kerusakan ginjal dan kanker.\nChloramine : Kandungan chloramine dalam air. Chloramine adalah senyawa kimia yang terbentuk dari klorin dan amonia. Ini digunakan sebagai desinfektan dalam air minum. berbahaya jika lebih besar dari 4. Paparan kloramine dalam jumlah yang tinggi dapat menyebabkan iritasi mata dan tenggorokan.\nChromium : Kandungan kromium dalam air. berbahaya jika lebih besar dari 0,1. Pemaparan kromium VI dapat menyebabkan kerusakan paru-paru, penyakit pernapasan, dan kanker.\nCopper : Kandungan tembaga dalam air. Copper adalah unsur kimia dengan simbol Cu dan nomor atom 29. Copper digunakan dalam instalasi listrik, pipa, dan peralatan masak. berbahaya jika lebih besar dari 1,3. Kandungan tembaga yang berlebihan dalam air minum dapat menyebabkan gangguan pencernaan dan masalah hati.\nFluoride : Kandungan fluoride dalam air. Fluoride adalah ion anorganik yang penting untuk kesehatan gigi. berbahaya jika lebih besar dari 1,5. konsumsi fluoride dalam jumlah yang berlebihan dapat menyebabkan masalah kesehatan gigi dan tulang.\nBacteria : Indikator keberadaan bakteri dalam air. Bakteri adalah mikroorganisme yang dapat ditemukan dalam air. berbahaya jika lebih besar dari 0.\nViruses : Indikator keberadaan virus dalam air. berbahaya jika lebih besar dari 0\nLead : Kandungan timbal dalam air. Lead adalah logam berat yang dapat menyebabkan keracunan, terutama pada anak-anak. berbahaya jika lebih besar dari 0,015. Pemaparan timbal dapat menyebabkan kerusakan otak dan sistem saraf.\nNitrates : Kandungan nitrat dalam air. Nitrates adalah senyawa kimia yang dapat ditemukan dalam pupuk dan limbah industriberbahaya jika lebih besar dari 10. Kandungan nitrates yang tinggi dalam air dapat menyebabkan masalah kesehatan, terutama pada bayi.\nNitrites : Kandungan nitrit dalam air. nitrites adalah senyawa kimia yang dapat ditemukan dalam pupuk dan limbah industriberbahaya jika lebih besar dari 1. Kandungan nitrites yang tinggi dalam air dapat menyebabkan masalah kesehatan, terutama pada bayi.\nMercury : Kandungan merkuri dalam air. Mercury adalah logam berat yang dapat mengakumulasi dalam organisme hidup dan menyebabkan keracunan. berbahaya jika lebih besar dari 0,002. Pemaparan merkuri dapat merusak otak, ginjal, dan sistem saraf.\nPerchlorate : Kandungan perchlorate dalam air. Perchlorate adalah senyawa kimia yang digunakan dalam bahan peledak dan propelan roket. Pemaparan perchlorate dapat mengganggu fungsi tiroid. berbahaya jika lebih besar dari 56\nRadium : Kandungan radium dalam air. Radium adalah unsur radioaktif yang dapat ditemukan secara alami dalam tanah dan air. Paparan radium dapat meningkatkan risiko kanker. berbahaya jika lebih besar dari 5\nSelenium : Kandungan selenium dalam air. berbahaya jika lebih besar dari 0,5. konsumsi selenium yang berlebihan dapat menyebabkan masalah kesehatan, termasuk kerusakan saraf.\nSilver : Kandungan perak dalam air. berbahaya jika lebih besar dari 0,1. Konsumsi perak dalam jumlah yang berlebihan dapat menyebabkan argyria, kondisi di mana kulit manusia berubah menjadi warna biru keabu-abuan.\nUranium : Kandungan uranium dalam air. Uranium adalah unsur radioaktif yang dapat ditemukan secara alami dalam batuan dan air. Paparan uranium dapat meningkatkan risiko kanker dan masalah ginjal. berbahaya jika lebih besar dari 0,3\nIs_safe : Kolom ini adalah label atau target variabel yang menunjukkan apakah sampel air tersebut aman untuk dikonsumsi atau tidak. class attribute {0 - not safe, 1 - safe}\n\nfitur di atas ini mencerminkan kandungan berbagai mikroorganisme dalam air. Dalam setiap fitur atau kandungan yang ada dalam air tersebut memiliki batasan aman. jika kandungan mikroorganisme melebihi nilai-nilai batasan ini, air dianggap tidak aman untuk konsumsi manusia.\n\ndf.columns\n\nIndex(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium', 'is_safe'],\n      dtype='object')\n\n\nfungsi columns digunakan untuk menampilkan nama-nama kolom pada dataframe. yang mana dataset water quality ini terdiri dari 21 kolom, yakni aluminium, ammonia, arsenic, barium, cadmium, chloramine, chromium, copper, flouride, bacteria, viruses, lead, nitrates, nitrites, mercury, perchlorate, radium, selenium, silver, uranium, dan is_safe.\n\ndf.describe()\n\n\n\n\n\n\n\n\naluminium\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\nviruses\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\n\n\n\n\ncount\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n7999.000000\n\n\nmean\n0.666158\n0.161445\n1.567715\n0.042806\n2.176831\n0.247226\n0.805857\n0.771565\n0.319665\n0.328583\n0.099450\n9.818822\n1.329961\n0.005194\n16.460299\n2.920548\n0.049685\n0.147781\n0.044673\n\n\nstd\n1.265145\n0.252590\n1.216091\n0.036049\n2.567027\n0.270640\n0.653539\n0.435373\n0.329485\n0.378096\n0.058172\n5.541331\n0.573219\n0.002967\n17.687474\n2.323009\n0.028770\n0.143551\n0.026904\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.040000\n0.030000\n0.560000\n0.008000\n0.100000\n0.050000\n0.090000\n0.405000\n0.000000\n0.002000\n0.048000\n5.000000\n1.000000\n0.003000\n2.170000\n0.820000\n0.020000\n0.040000\n0.020000\n\n\n50%\n0.070000\n0.050000\n1.190000\n0.040000\n0.530000\n0.090000\n0.750000\n0.770000\n0.220000\n0.008000\n0.102000\n9.930000\n1.420000\n0.005000\n7.740000\n2.410000\n0.050000\n0.080000\n0.050000\n\n\n75%\n0.280000\n0.100000\n2.480000\n0.070000\n4.240000\n0.440000\n1.390000\n1.160000\n0.610000\n0.700000\n0.151000\n14.610000\n1.760000\n0.008000\n29.480000\n4.670000\n0.070000\n0.240000\n0.070000\n\n\nmax\n5.050000\n1.050000\n4.940000\n0.130000\n8.680000\n0.900000\n2.000000\n1.500000\n1.000000\n1.000000\n0.200000\n19.830000\n2.930000\n0.010000\n60.010000\n7.990000\n0.100000\n0.500000\n0.090000\n\n\n\n\n\n\n\nFungsi describe() digunakan untuk menampilkan statistik deskriptif dari data frame atau series. Output dari fungsi ini berisi rangkuman central tendency dan sebaran dari dataset. Fungsi describe() membantu kita untuk mendapatkan overview dari dataset. Di bawah ini adalah penjelasan hasil yang diberikan:\n\nCount (Jumlah): Menunjukkan jumlah data yang ada dalam setiap kolom. Misalnya, pada kolom “aluminium”, terdapat 7999 data.\nMean (Rata-rata): Menunjukkan nilai rata-rata dari setiap kolom. Misalnya, rata-rata kandungan aluminium adalah sekitar 0.666158.\nStd (Standar Deviasi): Menunjukkan sejauh mana nilai-nilai dalam kolom tersebar dari rata-rata. Semakin tinggi standar deviasi, semakin besar variabilitasnya dari rata-rata. Misalnya, standar deviasi kandungan aluminium adalah sekitar 1.265145.\nMin (Minimum): Menunjukkan nilai terkecil dalam setiap kolom. Misalnya, nilai minimum kandungan arsenik adalah 0.0.\n25% (Kuartil Pertama): Menunjukkan nilai yang membagi 25% data terendah dari nilai-nilai lainnya. Misalnya, pada kolom “aluminium”, 25% data berada di bawah 0.040000.\n50% (Median atau Kuartil Kedua): Menunjukkan nilai yang membagi dataset menjadi dua bagian setara, atau median. Misalnya, median kandungan aluminium adalah 0.070000.\n75% (Kuartil Ketiga): Menunjukkan nilai yang membagi 75% data terendah dari nilai-nilai lainnya. Misalnya, pada kolom “aluminium”, 75% data berada di bawah 0.280000.\nMax (Maksimum): Menunjukkan nilai tertinggi dalam setiap kolom. Misalnya, nilai maksimum kandungan aluminium adalah 5.050000."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-tipe-data-fitur",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#deskripsi-tipe-data-fitur",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.3 Deskripsi Tipe Data Fitur",
    "text": "2.3 Deskripsi Tipe Data Fitur\nBerikut adalah analisis tipe data untuk setiap kolom beserta alasannya:\n\naluminium: Tipe data rasio. Kandungan aluminium dalam air memiliki nol yang bermakna, dan perbandingan antara dua nilai memiliki arti yang jelas (misalnya, 2 kali lipat).\nammonia: Tipe data rasio. Kandungan ammonia dalam air juga memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\narsenic: Tipe data rasio. Kandungan arsenik dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nbarium: Tipe data rasio. Kandungan barium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\ncadmium: Tipe data rasio. Kandungan cadmium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nchloramine: Tipe data rasio. Kandungan chloramine dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nchromium: Tipe data rasio. Kandungan chromium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\ncopper: Tipe data rasio. Kandungan copper dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nflouride: Tipe data rasio. Kandungan flouride dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nbacteria: Tipe data rasio. Kandungan bakteri dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nviruses: Tipe data rasio. Kandungan virus dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nlead: Tipe data rasio. Kandungan lead dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nnitrates: Tipe data rasio. Kandungan nitrates dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nnitrites: Tipe data rasio. Kandungan nitrites dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nmercury: Tipe data rasio. Kandungan mercury dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nperchlorate: Tipe data rasio. Kandungan perchlorate dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nradium: Tipe data rasio. Kandungan radium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nselenium: Tipe data rasio. Kandungan selenium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nsilver: Tipe data rasio. Kandungan silver dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nuranium: Tipe data rasio. Kandungan uranium dalam air memiliki nol yang bermakna dan perbandingan antara dua nilai memiliki arti yang jelas.\nis_safe: Tipe data nominal. Variabel target ini merupakan label kategori yang menunjukkan apakah air layak diminum atau tidak. Ini merupakan tipe data kategorikal dengan dua kategori yang bersifat nominal."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-missing-value",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-missing-value",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.4 Identifikasi Missing Value",
    "text": "2.4 Identifikasi Missing Value\n(Dataset ini memiliki 3 missing value pada kolom amonia dan is_safe) penjelasannya sebagai berikut :\nIdentifikasi missing value adalah proses mengenali dan menentukan lokasi di mana nilai-nilai yang hilang (missing value) terdapat dalam dataset. untuk mencari missing value kita bisa menggunakan fungsi info(). Fungsi info() digunakan untuk menampilkan informasi detail tentang dataframe, seperti jumlah baris data, nama-nama kolom berserta jumlah data dan tipe datanya, dan sebagainya. Dimana didapat bahwa dataset tersebut nampaknya tidak memiliki nilai yang missing karena nilai non-null setiap kolomnya sebanyak 7999 sama dengan jumlah dataset sebenarnya. akan tetapi perlu di telisik lebih dalam karena bisa jadi valuenya terdapat kerusakan seperti yang awalnya berupa angka, akan tetapi berubah menjadi stringg atau object, dimana itu terdeteksi pada kolom amonia dan is_safe.\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nRangeIndex: 7999 entries, 0 to 7998\nData columns (total 21 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   aluminium    7999 non-null   float64\n 1   ammonia      7999 non-null   object \n 2   arsenic      7999 non-null   float64\n 3   barium       7999 non-null   float64\n 4   cadmium      7999 non-null   float64\n 5   chloramine   7999 non-null   float64\n 6   chromium     7999 non-null   float64\n 7   copper       7999 non-null   float64\n 8   flouride     7999 non-null   float64\n 9   bacteria     7999 non-null   float64\n 10  viruses      7999 non-null   float64\n 11  lead         7999 non-null   float64\n 12  nitrates     7999 non-null   float64\n 13  nitrites     7999 non-null   float64\n 14  mercury      7999 non-null   float64\n 15  perchlorate  7999 non-null   float64\n 16  radium       7999 non-null   float64\n 17  selenium     7999 non-null   float64\n 18  silver       7999 non-null   float64\n 19  uranium      7999 non-null   float64\n 20  is_safe      7999 non-null   object \ndtypes: float64(19), object(2)\nmemory usage: 1.3+ MB\n\n\nKita akan mengecek apakah sebuah data mengandung Pesan “#NUM!”. #NUM! mungkin muncul karena adanya kesalahan atau format yang tidak tepat dalam dataset, terutama pada kolom yang seharusnya berisi nilai numerik. Tanda ini seringkali menunjukkan bahwa suatu sel atau entri dalam dataset tidak dapat diuraikan sebagai nilai numerik yang valid.\n\n# Menghitung apakah ada nilai '#NUM!' dalam setiap kolom\ncontains_num = df.eq('#NUM!')\n\n# Menampilkan hasil\nprint(\"Apakah ada nilai '#NUM!' dalam setiap kolom:\")\nprint(contains_num.any())\n\n# Menghitung jumlah nilai '#NUM!' untuk setiap kolom\nnum_count_per_column = contains_num.sum()\nprint(\"Jumlah nilai '#NUM!' untuk setiap kolom:\")\nprint(num_count_per_column)\n\nApakah ada nilai '#NUM!' dalam setiap kolom:\naluminium      False\nammonia         True\narsenic        False\nbarium         False\ncadmium        False\nchloramine     False\nchromium       False\ncopper         False\nflouride       False\nbacteria       False\nviruses        False\nlead           False\nnitrates       False\nnitrites       False\nmercury        False\nperchlorate    False\nradium         False\nselenium       False\nsilver         False\nuranium        False\nis_safe         True\ndtype: bool\nJumlah nilai '#NUM!' untuk setiap kolom:\naluminium      0\nammonia        3\narsenic        0\nbarium         0\ncadmium        0\nchloramine     0\nchromium       0\ncopper         0\nflouride       0\nbacteria       0\nviruses        0\nlead           0\nnitrates       0\nnitrites       0\nmercury        0\nperchlorate    0\nradium         0\nselenium       0\nsilver         0\nuranium        0\nis_safe        3\ndtype: int64\n\n\nKode di atas ini digunakan untuk mengakumulasi dengan melakukan pengecekan apakah terdapat nilai yang hilang (NaN) dalam kolom yang ada dalam DataFrame (df). - Fungsi eq (singkatan dari “equal”) pada Pandas digunakan untuk membandingkan elemen-elemen dalam suatu DataFrame atau Series dengan nilai tertentu dan mengembalikan DataFrame atau Series baru yang berisi nilai True jika elemennya sama dengan nilai yang dibandingkan, dan False jika tidak. - Fungsi any() fungsi any() pada Pandas digunakan untuk menentukan apakah setidaknya satu elemen dalam suatu DataFrame atau Series memiliki nilai True. Jika ada setidaknya satu nilai True, maka fungsi any() akan mengembalikan True; sebaliknya, jika semua nilai adalah False, maka akan mengembalikan False. - fungsi sum() fungsi sum() pada pandas digunakan untuk menjumlahkan suatu data, dimana fungsi ini digunakan untuk menghitung jumlah nilai True (jumlah nilai ‘#NUM!’) dalam setiap kolom dan menyimpannya dalam suatu variable untuk kemudian ditampilkan.\nBerdasarkan identifikasi lebih dalam maka didapati nilai yang hilang berupa #NUM! Dalam dataset water quality ini memiliki 3 missing value pada kolom amonia dan is_safe."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-data-duplicated",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-data-duplicated",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.5 Identifikasi Data Duplicated",
    "text": "2.5 Identifikasi Data Duplicated\nIdentifikasi data duplikat merujuk pada proses menemukan dan menandai baris atau entri dalam dataset yang memiliki nilai yang sama di semua kolomnya. Dengan kata lain, data duplikat adalah duplikat persis dari baris lainnya dalam dataset. Dalam dataset water Quality ini tidak ditemukannya data duplikat.\n\njumlah_duplikat = df.duplicated().sum()\n\n# Menampilkan jumlah data yang duplikat\nprint(\"Jumlah data yang duplikat:\", jumlah_duplikat)\n\nJumlah data yang duplikat: 0\n\n\nKode diatas yakni digunakan untuk mengidentifikasi dan menghitung jumlah data duplikat dalam DataFrame (df). Menggunakan metode duplicated() untuk menghasilkan keterangan boolean yang menunjukkan apakah setiap baris dalam DataFrame adalah duplikat dari baris sebelumnya atau tidak (True atau False). Kemudian, metode sum() digunakan untuk menghitung jumlah total nilai True, yang merupakan jumlah total data duplikat. Dalam dataset saya, tidak ada data yang mengalami duplikasi data.\nContoh deteksi duplikat data : | A | B | |—|——–| | 1 | apple | | 2 | banana | | 3 | apple | | 2 | banana | | 4 | orange |\nBaris-baris yang merupakan duplikat: - 0 False - 1 False - 2 False - 3 True - 4 False"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-sebaran-kelas-data",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-sebaran-kelas-data",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.6 Identifikasi Sebaran Kelas Data",
    "text": "2.6 Identifikasi Sebaran Kelas Data\nidentifikasi ini digunakan untuk melihat sejauh mana perbedaan jumlah kedua kelas yakni kelas 0 atau tidak aman, dan kelas 1 atau aman. Dalam dataset Water Quality ini mengalamni unbalance data, dimana terdapat perbedaan yang cukup signifikan dalam jumlah data tiap kelasnya. Data dengan kelas 0 atau tidak aman berjumlah 7084 berbanding cukup jauh dengan data dengan kelas 1 yang hanya berkisar 912 data saja. Oleh karena itu, perlu dilakukan pre-processing untuk menyamaratakan jumlah data setiap kelasnya.\nMenurut He dan Edwardo (2009) sebuah himpunan data dikatakan imbalanced jika terdapat salah satu kelas yang direpresentasikan dalam jumlah yang tidak sebanding dengan kelas yang lain. Kondisi imbalanced data menjadi masalah dalam klasifikasi karena classifier learning akan condong memprediksi ke kelas data yang banyak (mayoritas) dibanding dengan kelas yang sedikit (minoritas). Akibatnya, dihasilkan akurasi prediksi yang baik terhadap kelas data training yang banyak (kelas mayoritas) sedangkan untuk kelas data training yang sedikit (kelas minoritas) akan dihasilkan akurasi prediksi yang buruk.\nImbalanced data dapat diatasi dengan beberapa cara, antara lain dengan pengambilan sampel pada setiap kelas dan strategi sampling seperti oversampling atau undersampling.\n\ndf = pd.read_csv('waterQuality1.csv')\n\n\nprint(df['is_safe'].unique())\nprint(df.is_safe.value_counts())\n\n['1' '0' '#NUM!']\n0        7084\n1         912\n#NUM!       3\nName: is_safe, dtype: int64\n\n\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Menggunakan seaborn untuk membuat countplot\nsns.set(style=\"whitegrid\")\nax = sns.countplot(data=df, x='is_safe')\n\n# Menambahkan label pada sumbu y\nplt.ylabel('Jumlah Data')\n\n# Menambahkan judul plot\nplt.title('Distribusi Kelas')\n\n# Menampilkan jumlah data untuk masing-masing kelas di atas batangnya\nfor p in ax.patches:\n    ax.annotate(f'{p.get_height()}', (p.get_x() + p.get_width() / 2., p.get_height()),\n                ha='center', va='center', xytext=(0, 10), textcoords='offset points')\n\n# Menampilkan plot\nplt.show()\n\n\n\n\nKode diatas menggunakan seaborn dan matplotlib untuk membuat countplot dari kolom ‘is_safe’ dalam DataFrame df. Hasilnya adalah visualisasi distribusi kelas ‘is_safe’ dengan jumlah data di setiap kategori."
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-outlier",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#identifikasi-outlier",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.7 Identifikasi Outlier",
    "text": "2.7 Identifikasi Outlier\nIdentifikasi Outlier kali ini menggunakan Metode Local Outlier Factor. Local Outlier Factor (LOF) adalah metode yang digunakan untuk mendeteksi outlier dalam dataset. Data dalam dataset water quality yang mengalami outlier adalah sebanyak 1999 data.\nMetode Local Outlier Factor (LOF) membandingkan kerapatan suatu titik data dengan kerapatan titik-titik di sekitarnya. LOF mengukur sejauh mana suatu titik data dianggap anomali atau outlier berdasarkan perbedaan kerapatan. Metode ini dikembangkan oleh Markus M. Breunig, Hans-Peter Kriegel, Raymond T. Ng, dan Jörg Sander pada tahun 2000. Sumber\nBerikut adalah langkah-langkah Local Outlier Factor (LOF): 1. Hitung jarak antar titik dan tetangga - Rumus jarak euclidean antara dua titik p dan 1 dalam dimensi n: \\[\\begin{equation}\n\\text{Euclidean Distance} (P, Q) = \\sqrt{\\sum_{i=1}^{n} (q_i - p_i)^2}\n\\end{equation}\\] \\[\\begin{equation}\n\\text{Euclidean Distance} (P, Q) = \\sqrt{(q_1 - p_1)^2 + (q_2 - p_2)^2}\n\\end{equation}\\]\n\nHitung kepadatan lokasi\n\n\nKepadatan lokal suatu titik p: \\[\\begin{equation}\n\\text{Density}_p = \\frac{1}{{\\text{k-th distance}_p}}\n\\end{equation}\\] \\[\\begin{equation}\n{{\\text{Dimana k-th distance adalah jarak ke titik tetangga ke-k}_p}}\n\\end{equation}\\]\n\n\nHitung Local Reachability Density (LRD)\n\n\nLocal Reachability Density (LRD) suatu titik p: \\[\\begin{equation}\n\\text{LRD}_p = \\left(\\text{Density}_p\\right)^{\\frac{1}{\\alpha}}\n\\end{equation}\\] di mana α adalah parameter untuk menyesuaikan sensitivitas terhadap fluktuasi kecil dalam kepadatan.\n\n\nHitung Local Outlier Factor (LOF)\n\n\nLocal Outlier Factor (LOF) suatu titik p: \\[\\begin{equation}\n\\text{LOF}_p = \\frac{\\left|\\text{Neighbors}_p\\right|}{\\sum_{o \\in \\text{Neighbors}_p} \\frac{\\text{LRD}_o}{\\text{LRD}_p}}\n\\end{equation}\\] di mana Neighborsp adalah himpunan tetangga dari titik p.\n\n\nIdentifikasi Outlier: Tentukan threshold, dan labeli titik-titik yang memiliki LOF di atas threshold sebagai outlier. Pada umumnya, jika LOF(p) &gt; Threshold, maka titik p dianggap sebagai outlier.\n\n\ndf = pd.read_csv('waterQuality1.csv')\n\n\nimport pandas as pd\nfrom sklearn.neighbors import LocalOutlierFactor\n\n# Mengganti '#NUM!' dengan NaN dan mengonversi kolom ke tipe float\ndf.replace('#NUM!', float('nan'), inplace=True)\ndf = df.apply(pd.to_numeric, errors='coerce')\n\n# Filter DataFrame untuk nilai yang bukan NaN (numerik)\ndf_numeric = df.dropna()\n\n# Contamination values yang ingin diuji\ncontamination_values = [0.05, 0.1, 0.15, 0.2, 0.25]\n\n# Inisialisasi variabel untuk menyimpan hasil terbaik\nmax_outliers = 0\nbest_params = {'n_neighbors': None, 'contamination': None}\n\n# Loop untuk nilai n_neighbors dari 1 hingga 50\nfor n_neighbors in range(1, 51):\n    for contamination in contamination_values:\n        lof = LocalOutlierFactor(n_neighbors=n_neighbors, contamination=contamination)\n        outlier_labels = lof.fit_predict(df_numeric)\n        num_outliers = sum(outlier_labels == -1)  # Menghitung jumlah outlier yang dideteksi\n\n        # Memperbarui hasil terbaik jika ditemukan jumlah outlier yang lebih banyak\n        if num_outliers &gt; max_outliers:\n            max_outliers = num_outliers\n            best_params['n_neighbors'] = n_neighbors\n            best_params['contamination'] = contamination\n\n# Mencetak hasil terbaik\nprint(\"Parameter terbaik:\")\nprint(\"Jumlah data outlier :\", max_outliers)\nprint(\"Jumlah data tanpa outlier :\", (df.shape[0])-max_outliers)\nprint(\"n_neighbors =\", best_params['n_neighbors'])\nprint(\"contamination =\", best_params['contamination'])\n\nC:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\nfound 0 physical cores &lt; 1\nReturning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n  warnings.warn(\n  File \"C:\\Users\\Lenovo\\anaconda3\\Lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n    raise ValueError(f\"found {cpu_count_physical} physical cores &lt; 1\")\n\n\nParameter terbaik:\nJumlah data outlier : 1999\nJumlah data tanpa outlier : 6000\nn_neighbors = 1\ncontamination = 0.25\n\n\nPada kode di atas, saya menerapkan grid search untuk mencari parameter yang bagus guna mendeteksi outlier, dimana parameter yang terpilih yakni dengan n_neighbors 1 dan contaminationsnya 0,25"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#eksplorasi-data",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#eksplorasi-data",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "2.8 Eksplorasi Data",
    "text": "2.8 Eksplorasi Data\nPada eksplorasi data kali ini saya akan menampilkan data ke dalam bentuk grafik-grafik seperti histogram, dan matrix. Visualisasi ini agar memudahkan untuk mengambil informasi penting dari sebuah data.\n\n# Membaca data dari file csv\ndf = pd.read_csv('waterQuality1.csv')\n\n\n2.8.1 Histogram Sebaran Frekuensi Data Setiap Kolom\n\n\n#distribution data\ndf.hist(figsize=(14, 14))\nplt.show()\n\n\n\n\nKode diatas bertujuan untuk memberikan visualisasi distribusi data dari dataframe df menggunakan histogram. Histogram adalah grafik yang membagi rentang data ke dalam interval dan menghitung jumlah data yang jatuh dalam setiap interval tersebut. Ini membantu untuk memahami pola distribusi data dan melihat sebaran nilai-nilai dalam dataset.\n\nsns.set(style='whitegrid')\n\n# Plot histograms for the distribution of all columns\nplt.figure(figsize=(12, 15))\ndata_columns = df.columns.tolist()\ndata_columns.remove('is_safe')\n\n# Adjust the number of rows and columns in the subplot grid for better visualization\nnum_rows = len(data_columns) // 2 + len(data_columns) % 2\nnum_cols = 2\n\nfor i, column in enumerate(data_columns):\n    plt.subplot(num_rows, num_cols, i + 1)\n    sns.histplot(df[column], kde=True, color='orange', bins=40)\n    plt.title(f'Distribusi kolom {column}', fontsize=12)\n    plt.xlabel('')\n    plt.ylabel('Kepadatan')\n\nplt.suptitle(\"Distribusi Data\")\nplt.tight_layout()\nplt.show()\n\n\n\n\nKode di atas mengatur gaya plot, kemudian memplot histogram untuk distribusi semua kolom pada DataFrame. Setiap subplot menunjukkan distribusi nilai dari satu kolom, dan garis halus (kernel density estimation) ditambahkan untuk memberikan representasi kepadatan distribusi data.\n\n2.8.2 Matrix Korelasi\n\nKode berikut dibawah ini menghasilkan sebuah matriks korelasi dengan menggunakan fungsi corr() dari pandas pada DataFrame df. Matriks korelasi ini merepresentasikan hubungan linier antara semua pasangan variabel dalam dataset. Setiap sel dalam matriks menunjukkan nilai korelasi antara dua variabel.\n\ncorrelation_matrix = df.corr()\n\nplt.figure(figsize=(16, 15))\nsns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, vmin=-1, vmax=1)\nplt.title(\"Correlation Plot of Water Quality Parameters\")\nplt.show()\n\nC:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_13180\\1987230932.py:1: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n  correlation_matrix = df.corr()\n\n\n\n\n\nCara membaca matriks korelasi:\nSkala Nilai Korelasi: - 1.0: Korelasi sempurna positif. Artinya, jika satu variabel naik, yang lain juga naik secara linear. - -1.0: Korelasi sempurna negatif. Artinya, jika satu variabel naik, yang lain turun secara linear. - 0.0: Tidak ada korelasi linier antara variabel tersebut.\nWarna Sel pada Heatmap digunakan untuk memvisualisasikan matriks korelasi. Sel-sel dengan warna yang lebih terang atau lebih gelap menunjukkan nilai korelasi yang lebih tinggi, sementara warna yang lebih netral menunjukkan korelasi yang lebih rendah.\n\n2.8.3 Diagram Lingkaran perbandingan jumlah data kelas\n\nini digunakan untuk menggambarkan persentasi perbandingan distribusi data setiap kelas yakni kelas 0 (tidak aman) dan kelas 1 (aman)\n\nimport matplotlib.pyplot as plt\n\n# Menghapus baris dengan nilai '#NUM!'\ndf_cleaned = df[df != '#NUM!'].dropna()\n\n# Menghitung jumlah data untuk setiap kelas\nclass_counts = df_cleaned['is_safe'].value_counts()\n\n# Membuat plot pie chart\nplt.figure(figsize=(6, 6))\nplt.pie(class_counts, labels=class_counts.index, autopct='%1.1f%%', startangle=90, colors=['lightgreen', 'lightcoral'])\nplt.title('Distribusi Kelas (is_safe)')\nplt.show()"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#handling-missing-data",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#handling-missing-data",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.1 Handling Missing Data",
    "text": "3.1 Handling Missing Data\nMengidentifikasi dan menangani nilai-nilai yang hilang dalam dataset. Cara penanganannya bisa berupa menghapus baris/kolom yang mengandung nilai yang hilang atau mengisi nilai yang hilang dengan suatu nilai (misalnya, rata-rata, median, atau nilai yang paling sering muncul).\nSebelumnya telah teridentifikasi bahwa dataset water quality ini memiliki 3 nilai yang hilang atau berupa #NUM! pada kolom amonia dan is_safe, selain itu kolom tersebut yang masih berupa object bukan numeric. oleh karena itu perlu dilakukan penanganan dengan cara menghapus kolom yang memiliki nilai yang hilang tersebut.\n\n# Membaca data dari file csv\ndf = pd.read_csv('waterQuality1.csv')\n\nPertama kita ganti nilai #NUM! menjadi NaN. Fungsi replace() digunakan untuk mengganti sebuah nilai pada dataframe. Misalnya disini kita mengganti nilai #NUM! yang ada di dataframe dengan NaN.\n\ndf.replace(\"#NUM!\", pd.NA, inplace=True)\n\nSelanjutnya menseleksi dataframe yang kolom amonia-nya tidak mengandung missing value atau NaN. artinya kolom yang hilang tadi dilakukan penghapusan data.\n\ndf = df[df['ammonia'].notna()]\n\nLalu dilakukan penggantian tipe data kolom amonia dan is_safe yang sebelumnya berupa object atau string menjadi tipe data numeric menggunakan fungsi to_numeric().\n\ndf['ammonia'] = pd.to_numeric(df['ammonia'])\ndf['is_safe'] = pd.to_numeric(df['is_safe'])\n\nselanjutnya dilakukan pengecekan kembali terhadap dataset apakah missing value telah dihapus atau belum, dan melihat tipe data dari setiap kolomnya apakah sudah menjadi numeric semua.\n\n# Menghitung apakah ada nilai yang hilang dalam setiap kolom\nmissing_values = df.isna().any()\n\n# Menampilkan hasil\nprint(\"Apakah ada nilai yang hilang dalam setiap kolom:\")\nprint(missing_values)\n\nnan_kolom = df.isna().sum()\nprint(\"Jumlah nilai yang hilang (NA) untuk setiap kolom:\")\nprint(nan_kolom)\n\nApakah ada nilai yang hilang dalam setiap kolom:\naluminium      False\nammonia        False\narsenic        False\nbarium         False\ncadmium        False\nchloramine     False\nchromium       False\ncopper         False\nflouride       False\nbacteria       False\nviruses        False\nlead           False\nnitrates       False\nnitrites       False\nmercury        False\nperchlorate    False\nradium         False\nselenium       False\nsilver         False\nuranium        False\nis_safe        False\ndtype: bool\nJumlah nilai yang hilang (NA) untuk setiap kolom:\naluminium      0\nammonia        0\narsenic        0\nbarium         0\ncadmium        0\nchloramine     0\nchromium       0\ncopper         0\nflouride       0\nbacteria       0\nviruses        0\nlead           0\nnitrates       0\nnitrites       0\nmercury        0\nperchlorate    0\nradium         0\nselenium       0\nsilver         0\nuranium        0\nis_safe        0\ndtype: int64\n\n\n\ndf.info()\n\n&lt;class 'pandas.core.frame.DataFrame'&gt;\nInt64Index: 7996 entries, 0 to 7998\nData columns (total 21 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   aluminium    7996 non-null   float64\n 1   ammonia      7996 non-null   float64\n 2   arsenic      7996 non-null   float64\n 3   barium       7996 non-null   float64\n 4   cadmium      7996 non-null   float64\n 5   chloramine   7996 non-null   float64\n 6   chromium     7996 non-null   float64\n 7   copper       7996 non-null   float64\n 8   flouride     7996 non-null   float64\n 9   bacteria     7996 non-null   float64\n 10  viruses      7996 non-null   float64\n 11  lead         7996 non-null   float64\n 12  nitrates     7996 non-null   float64\n 13  nitrites     7996 non-null   float64\n 14  mercury      7996 non-null   float64\n 15  perchlorate  7996 non-null   float64\n 16  radium       7996 non-null   float64\n 17  selenium     7996 non-null   float64\n 18  silver       7996 non-null   float64\n 19  uranium      7996 non-null   float64\n 20  is_safe      7996 non-null   int64  \ndtypes: float64(20), int64(1)\nmemory usage: 1.3 MB\n\n\nMelalui pengecekan ini, didapati bahwa missing value telah dihapus, dan tipe data kolom sudah menjadi numeric dan bukan object. Selanjutnya kita bisa simpang hasil data yang sudah tidak ada missing value dengan nama datanomissing.csv\n\ndf.to_csv('datanomissing.csv', index=False)"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#data-cleaning",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#data-cleaning",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.2 Data Cleaning",
    "text": "3.2 Data Cleaning\nData cleaning kali ini akan Mendeteksi dan mengatasi noise atau outlier dalam data. Noise dapat muncul sebagai nilai yang ekstrim atau tidak sesuai dengan distribusi umum data. Dalam data cleaning ini saya akan menggunakan metode Local Outlier Factor. Didapati pada data understanding dimana outlier terdeteksi sebanyak 1999 data dan perlu dilakukan penghapusan data tersebut.\n\n# Membaca data dari file csv\ndf = pd.read_csv('datanomissing.csv')\n\nKita coba gambarkan data dan sebarannya melalui BoxPlot untuk mengetahui seperti apa data outlier dalam setiap kolom.\n\n# Create a list of numerical features and plot them\nlist_of_num_features = df.drop(columns=['is_safe'])  # DataFrame of numerical features\npalette_features = ['#E68753', '#409996']\nsns.set(rc={'axes.facecolor':'#ECECEC'})\n\n# Mengatur tata letak subplot\nnum_rows = 5\nnum_cols = 6\nfig, axes = plt.subplots(num_rows, num_cols, figsize=(14, 14), sharex=False, sharey=False)\n\n# Flatten array of subplots for ease of indexing\naxes = axes.flatten()\n\nfor idx, feature in enumerate(list_of_num_features.columns):\n    ax = axes[idx]  # Mengambil subplot yang sesuai\n    sns.boxplot(x='is_safe', y=feature, data=df, palette=palette_features, ax=ax)\n    ax.set_title(feature, fontsize=12, fontweight='bold', ha='center')\n\n# Sembunyikan subplot yang tidak digunakan\nfor i in range(len(list_of_num_features.columns), num_rows * num_cols):\n    fig.delaxes(axes[i])\n\n# Atur tata letak dan tampilkan plot\nplt.tight_layout()\nplt.show()\n\n\n\n\nKita coba kembali menggunakan local outlier factor untuk mendeteksi jumlah data outlier di dalam data menggunakan fungsi LocalOutlierFactor dengan parameter n_neigbors yakni 1 dan contamination yakni 0,25. Parameter ini didapat dari pencarian menggunakan grid search pada data undestanding di atas. Selanjutnya baris data yang terdeteksi sebagai outlier akan diberikan label -1 sedangkan yang bukan outlier akan diberi label 1\n\n# Menggunakan Local Outlier Factor\nlof = LocalOutlierFactor(n_neighbors=1, contamination=0.25)\noutlier_labels = lof.fit_predict(df)\ndf['Outlier'] = outlier_labels\ndf\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\nOutlier\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n1\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n7991\n0.05\n7.78\n0.00\n1.95\n0.040\n0.10\n0.03\n0.03\n1.37\n0.00\n...\n14.29\n1.00\n0.005\n3.57\n2.13\n0.09\n0.06\n0.03\n1\n1\n\n\n7992\n0.05\n24.22\n0.02\n0.59\n0.010\n0.45\n0.02\n0.02\n1.48\n0.00\n...\n10.27\n1.00\n0.001\n1.48\n1.11\n0.09\n0.10\n0.08\n1\n-1\n\n\n7993\n0.09\n6.85\n0.00\n0.61\n0.030\n0.05\n0.05\n0.02\n0.91\n0.00\n...\n15.92\n1.00\n0.000\n1.35\n4.84\n0.00\n0.04\n0.05\n1\n1\n\n\n7994\n0.01\n10.00\n0.01\n2.00\n0.000\n2.00\n0.00\n0.09\n0.00\n0.00\n...\n0.00\n0.00\n0.000\n0.00\n0.00\n0.00\n0.00\n0.00\n1\n-1\n\n\n7995\n0.04\n6.85\n0.01\n0.70\n0.030\n0.05\n0.01\n0.03\n1.00\n0.00\n...\n15.92\n1.00\n0.000\n1.35\n4.84\n0.00\n0.04\n0.05\n1\n1\n\n\n\n\n7996 rows × 22 columns\n\n\n\nSelanjutnya menghitung jumlah outlier di dalam dataset water quality atau data yang memiliki label -1, dan menemukan sebanyak 1999 data termasuk data outlier.\n\n# Menghitung jumlah outlier\ntotal_outliers = (df['Outlier'] == -1).sum()\nprint(\"Total Jumlah Outlier:\", total_outliers)\n\n# Menghitung jumlah outlier\ntotal_no_outliers = (df['Outlier'] == 1).sum()\nprint(\"Total Data Tanpa Outlier:\", total_no_outliers)\n\nTotal Jumlah Outlier: 1999\nTotal Data Tanpa Outlier: 5997\n\n\nSelanjutnya dilakukan penghapusan data terhadap data outlier, kemudian hasil data yang bebas outlier saya simpan sebagai csv dengan nama data_no_outlier.csv. Pada data yang bersih atau bebas outlier, didapati bahwa kelas 0 sebanyak 5265 sedangkan kelas 1 yakni sebanyak 732 data.\n\ndf_no_outliers = df[df['Outlier'] != -1]\ndf_no_outliers = df_no_outliers.drop(columns=['Outlier'])\ndf_no_outliers.to_csv('data_no_outliers.csv', index=False)\n\ndf = pd.read_csv('data_no_outliers.csv')\n# Menghitung jumlah target pada data tanpa outlier\nclassnooutlier = df['is_safe'].value_counts()\n\nprint(\"Jumlah target pada data tanpa outlier:\")\nprint(classnooutlier)\n\nJumlah target pada data tanpa outlier:\n0    5236\n1     761\nName: is_safe, dtype: int64"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#handling-imbalanced-data",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#handling-imbalanced-data",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.3 Handling Imbalanced Data",
    "text": "3.3 Handling Imbalanced Data\nBalancing kelas pada dataset ini menggunakan teknik Random Over-Sampling With imblearn. Random Over-Sampling With imblearn adalah salah satu teknik yang digunakan untuk menangani ketidakseimbangan kelas. Dalam metode ini, jumlah sampel dalam kelas minoritas ditingkatkan dengan menambahkan salinan acak dari sampel yang sudah ada dalam kelas tersebut. Metode Random Over-Sampling memungkinkan untuk mengatasi ketidakseimbangan kelas tanpa menghapus data dari kelas mayoritas, sehingga tidak ada informasi yang hilang dalam prosesnya.\nSebelum dilakukan preprocessing, data dengan kelas 0 sejumlah 7084 dan kelas 1 sebanyak 912. Akan tetapi setelah dilakukan preprocesing data dengan kelas 0 hanya sebanyak 5265 dan kelas 1 hanya 732 data. Meskipun begitu masih perlu dilakukan balancing kelas agar kelas menjadi proporsional satu sama lain.\n\ndf = pd.read_csv('data_no_outliers.csv')\n\n\n3.3.1 Perbandingan kelas data awal\n\nsns.countplot(data = df, x = 'is_safe')\nplt.show()\n# sebaran class\nclass_0 = df[df['is_safe'] == 0]\nclass_1 = df[df['is_safe'] == 1]\nprint('class 0 (Not Safe) :', class_0.shape)\nprint('class 1 (Safe)     :', class_1.shape)\n\n\n\n\nclass 0 (Not Safe) : (5236, 21)\nclass 1 (Safe)     : (761, 21)\n\n\n\n\n3.3.2 Proses Balancing Data\nProses Random Over Sampling adalah sebagai berikut:  - Identifikasi Kelas Minoritas dengan menentukan kelas yang memiliki jumlah sampel lebih sedikit dan dianggap sebagai kelas minoritas.\n\nHitung selisih antara jumlah sampel di kelas mayoritas dan kelas minoritas.\nPilih Sampel Acak Dari kelas minoritas, pilih sampel secara acak sebanyak selisih yang dihitung.\nDuplikasi sampel acak yang telah dipilih dan tambahkan ke dataset. Dengan kata lain, kita menambahkan kembali sampel-sampel ini ke kelas minoritas.\n\n\n# Separate features (X) and target variable (y)\nX = df.drop('is_safe', axis=1)\ny = df['is_safe']\n\n# Print original class distribution\nprint('Sebaran Data :')\nprint('Sebaran Data Kelas Awal:', Counter(y))\n\n# Initialize RandomOverSampler\nros = RandomOverSampler(random_state=42)\n\n# Apply Random Over-Sampling to balance the classes\nX_ros, y_ros = ros.fit_resample(X, y)\n\n# Print resampled class distribution\nprint('Sebaran Data Kelas Setelah Balancing:', Counter(y_ros))\n\n# Menampilkan jumlah data setelah balancing sampling\nprint('\\nJumlah keseluruhan data setelah balancing sampling :')\nprint('Features (X_ros) shape:', X_ros.shape)\nprint('Target (y_ros) shape:', y_ros.shape)\n\n#visualisasi perbandingan data kelas\n# Plot original class distribution\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nsns.countplot(data=df, x='is_safe')\nplt.title('Original Class')\n\n# Plot resampled class distribution after Random Over-Sampling\nplt.subplot(1, 2, 2)\nsns.countplot(data=pd.DataFrame({'is_safe': y_ros}), x='is_safe')\nplt.title('Random Over-Sampling With imblearn')\n\nplt.tight_layout()\nplt.show()\n\nSebaran Data :\nSebaran Data Kelas Awal: Counter({0: 5236, 1: 761})\nSebaran Data Kelas Setelah Balancing: Counter({1: 5236, 0: 5236})\n\nJumlah keseluruhan data setelah balancing sampling :\nFeatures (X_ros) shape: (10472, 20)\nTarget (y_ros) shape: (10472,)\n\n\n\n\n\nDan didapat data setelah dilakukan over sampling kedua kelas yakni 0 dan 1 menjadi sama sama sebanyak 5265 data. sehingga jumlah data keseluruhan menjadi 10530 data. Hasil data yang telah dilakukan balancing ini kemudian disimpan ke dalam file dengan nama data_balancing.csv\n\n# Mengonversi array NumPy ke DataFrame pandas\ndf_X_ros = pd.DataFrame(X_ros, columns=X.columns)\ndf_y_ros = pd.DataFrame(y_ros, columns=['is_safe'])\n# Menggabungkan DataFrame X_ros dan y_ros\ndf_resampled = pd.concat([df_X_ros, df_y_ros], axis=1)\n\n# Menyimpan DataFrame ke dalam file Excel\ndf_resampled.to_csv('data_balancing.csv', index=False)\ndf_resampled\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10467\n1.55\n11.30\n0.02\n3.14\n0.007\n7.52\n0.03\n0.10\n1.38\n0.00\n...\n0.033\n7.43\n1.55\n0.007\n19.77\n2.42\n0.09\n0.26\n0.02\n1\n\n\n10468\n0.05\n29.28\n0.06\n0.40\n0.030\n0.07\n0.02\n1.05\n0.81\n0.00\n...\n0.066\n4.62\n0.90\n0.000\n2.08\n1.80\n0.07\n0.09\n0.02\n1\n\n\n10469\n0.39\n28.28\n0.44\n1.74\n0.120\n5.14\n0.05\n0.03\n1.29\n0.43\n...\n0.108\n9.90\n1.86\n0.004\n29.06\n2.12\n0.07\n0.07\n0.01\n1\n\n\n10470\n3.12\n18.12\n0.03\n3.98\n0.006\n1.87\n0.07\n0.66\n0.04\n0.32\n...\n0.028\n8.02\n1.07\n0.002\n40.27\n1.16\n0.00\n0.35\n0.02\n1\n\n\n10471\n1.14\n17.45\n0.35\n1.32\n0.040\n1.45\n0.38\n0.41\n0.72\n0.00\n...\n0.020\n1.27\n1.55\n0.010\n45.06\n2.97\n0.06\n0.10\n0.02\n1\n\n\n\n\n10472 rows × 21 columns"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#feature-scaling",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#feature-scaling",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.4 Feature Scaling",
    "text": "3.4 Feature Scaling\nFeature scaling (skala fitur) adalah teknik khusus di dalam data transformation yang mengubah nilai-nilai variabel ke dalam rentang tertentu, sehingga memastikan bahwa seluruh fitur memiliki skala yang serupa. Feature scaling sendiri bisa disebut dengan normalisasi data. dalam feature scaling kali ini saya akan memilih menggunakan metode robust scaler.\nMetode Normalisasi Robust Scaler adalah salah satu teknik normalisasi data yang tahan terhadap outlier.\nRobust Scaler mengubah setiap nilai dalam suatu fitur dengan menggunakan rumus berikut: \\[\\begin{equation}\nX_{\\text{scaled}} = \\frac{{X - \\text{median}}}{{Q_3 - Q_1}}\n\\end{equation}\\] Dimana : - Xscaled : adalah nilai yang telah diubah skala, - X : adalah nilai asli, - Q1 : adalah kuartil pertama (25th percentile), - Q3 : adalah kuartil ketiga (75th percentile), - Median : Nilai Tengah.\nBerikut adalah proses normalisasi menggunakan Robust Scaler dan contoh kasusnya:\nProses Normalisasi menggunakan Robust Scaler: 1. Hitung Median dan Kuartil: - Median (Q2): Nilai tengah dari set data. - Kuartil 1 (Q1): Nilai tengah antara nilai terkecil dan median. - Kuartil 3 (Q3): Nilai tengah antara median dan nilai terbesar. 2. Hitung Rentang Interkuartil (IQR): - IQR = Q3 - Q1. 3. Hitung Skala Robust: - Skala Robust = (X - Median) / IQR. Contoh Kasus: Misalkan kita memiliki dataset sebagai berikut:\n[10, 15, 20, 25, 30, 100]\nLangkah-langkah Normalisasi menggunakan Robust Scaler: 1. Hitung Median (Q2): Median = 22.5 2. Hitung Kuartil 1 (Q1): Q1 = 15 3. Hitung Kuartil 3 (Q3): Q3 = 30 4. Hitung IQR: IQR = Q3 - Q1 = 15 5. Hitung Skala Robust: Skala Robust = (X - Median) / IQR - Untuk nilai 10: (10 - 22.5) / 15 = -0.8333 - Untuk nilai 15: (15 - 22.5) / 15 = -0.5 - Untuk nilai 20: (20 - 22.5) / 15 = -0.3333 - Untuk nilai 25: (25 - 22.5) / 15 = 0.1667 - Untuk nilai 30: (30 - 22.5) / 15 = 0.5 - Untuk nilai 100: (100 - 22.5) / 15 = 4.5\nSehingga, dataset yang telah dinormalisasi menggunakan Robust Scaler adalah:\n[-0.8333, -0.5, -0.3333, 0.1667, 0.5, 4.5]\n\nMetode Robust Scaler ini memiliki keuntungan:\n\nTahan terhadap outlier: Robust Scaler lebih tahan terhadap outlier karena menggunakan informasi dari kuartil, yang kurang dipengaruhi oleh nilai ekstrem.\nKonservatif: Dengan menggunakan median dan IQR, metode ini dapat menghasilkan normalisasi yang lebih konservatif daripada Min-Max Scaling.\n\nMetode Robust Scaler Cocok Digunakan Jika:\n\nData mengandung nilai ekstrem atau outlier.\nDistribusi data tidak terdistribusi normal.\nSkala fitur bervariasi secara signifikan.\n\nPerbandingan dengan Min-Max Scaling dan Z-Score Scaling:\n\nMin-Max Scaling: Rentang nilai diubah menjadi 0 hingga 1. Rentang ini dapat sangat dipengaruhi oleh nilai outlier. Jika outlier signifikan, rentang nilai mungkin menjadi terlalu sempit.\nZ-Score Scaling (Standardization): Data diubah sehingga memiliki rata-rata 0 dan deviasi standar 1. Metode ini sensitif terhadap nilai ekstrem dan tidak sebaik Robust Scaler ketika ada outlier.\n\n\n\ndf = pd.read_csv('data_balancing.csv')\nX = df.drop('is_safe', axis=1)\ny = df['is_safe']\n# Inisialisasi RobustScaler\nRobustScaler = RobustScaler()\n\n# Fit dan transformasi data menggunakan RobustScaler\ndf_scaled = RobustScaler.fit_transform(X)\n\n# Membuat DataFrame baru dengan data yang telah diubah skala\nX = pd.DataFrame(X, columns=X.columns)\ny = pd.DataFrame(y, columns=['is_safe'])\ndatascled = pd.concat([X, y], axis=1)\ndf_normalized = pd.DataFrame(datascled)\ndf_normalized.to_csv('data_normalisasi.csv', index=False)\n\n# Menampilkan hasil\nprint(\"Original Data:\")\ndf\n\nOriginal Data:\n\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10467\n1.55\n11.30\n0.02\n3.14\n0.007\n7.52\n0.03\n0.10\n1.38\n0.00\n...\n0.033\n7.43\n1.55\n0.007\n19.77\n2.42\n0.09\n0.26\n0.02\n1\n\n\n10468\n0.05\n29.28\n0.06\n0.40\n0.030\n0.07\n0.02\n1.05\n0.81\n0.00\n...\n0.066\n4.62\n0.90\n0.000\n2.08\n1.80\n0.07\n0.09\n0.02\n1\n\n\n10469\n0.39\n28.28\n0.44\n1.74\n0.120\n5.14\n0.05\n0.03\n1.29\n0.43\n...\n0.108\n9.90\n1.86\n0.004\n29.06\n2.12\n0.07\n0.07\n0.01\n1\n\n\n10470\n3.12\n18.12\n0.03\n3.98\n0.006\n1.87\n0.07\n0.66\n0.04\n0.32\n...\n0.028\n8.02\n1.07\n0.002\n40.27\n1.16\n0.00\n0.35\n0.02\n1\n\n\n10471\n1.14\n17.45\n0.35\n1.32\n0.040\n1.45\n0.38\n0.41\n0.72\n0.00\n...\n0.020\n1.27\n1.55\n0.010\n45.06\n2.97\n0.06\n0.10\n0.02\n1\n\n\n\n\n10472 rows × 21 columns\n\n\n\n\nprint(\"\\nNormalized Data (Robust Scaling):\")\ndf_normalized\n\n\nNormalized Data (Robust Scaling):\n\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n10467\n1.55\n11.30\n0.02\n3.14\n0.007\n7.52\n0.03\n0.10\n1.38\n0.00\n...\n0.033\n7.43\n1.55\n0.007\n19.77\n2.42\n0.09\n0.26\n0.02\n1\n\n\n10468\n0.05\n29.28\n0.06\n0.40\n0.030\n0.07\n0.02\n1.05\n0.81\n0.00\n...\n0.066\n4.62\n0.90\n0.000\n2.08\n1.80\n0.07\n0.09\n0.02\n1\n\n\n10469\n0.39\n28.28\n0.44\n1.74\n0.120\n5.14\n0.05\n0.03\n1.29\n0.43\n...\n0.108\n9.90\n1.86\n0.004\n29.06\n2.12\n0.07\n0.07\n0.01\n1\n\n\n10470\n3.12\n18.12\n0.03\n3.98\n0.006\n1.87\n0.07\n0.66\n0.04\n0.32\n...\n0.028\n8.02\n1.07\n0.002\n40.27\n1.16\n0.00\n0.35\n0.02\n1\n\n\n10471\n1.14\n17.45\n0.35\n1.32\n0.040\n1.45\n0.38\n0.41\n0.72\n0.00\n...\n0.020\n1.27\n1.55\n0.010\n45.06\n2.97\n0.06\n0.10\n0.02\n1\n\n\n\n\n10472 rows × 21 columns"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#splitting-data",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#splitting-data",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.5 Splitting Data",
    "text": "3.5 Splitting Data\nMemisahkan dataset menjadi set pelatihan dan set pengujian untuk evaluasi model.\n\ndf = pd.read_csv('data_normalisasi.csv')\n\n\nX = df.drop('is_safe', axis=1)\ny = df['is_safe']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#pycaret-seleksi-model",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#pycaret-seleksi-model",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.6 Pycaret Seleksi Model",
    "text": "3.6 Pycaret Seleksi Model\n\nfrom pycaret.regression import *\nfrom pycaret.classification import *\n\n\nclf = setup(df, target='is_safe')\n\n\n\n\n\n\n \nDescription\nValue\n\n\n\n\n0\nSession id\n6165\n\n\n1\nTarget\nis_safe\n\n\n2\nTarget type\nBinary\n\n\n3\nOriginal data shape\n(10472, 21)\n\n\n4\nTransformed data shape\n(10472, 21)\n\n\n5\nTransformed train set shape\n(7330, 21)\n\n\n6\nTransformed test set shape\n(3142, 21)\n\n\n7\nNumeric features\n20\n\n\n8\nPreprocess\nTrue\n\n\n9\nImputation type\nsimple\n\n\n10\nNumeric imputation\nmean\n\n\n11\nCategorical imputation\nmode\n\n\n12\nFold Generator\nStratifiedKFold\n\n\n13\nFold Number\n10\n\n\n14\nCPU Jobs\n-1\n\n\n15\nUse GPU\nFalse\n\n\n16\nLog Experiment\nFalse\n\n\n17\nExperiment Name\nclf-default-name\n\n\n18\nUSI\nf125\n\n\n\n\n\n\nbest = clf.compare_models()\n\n\n\n\n\n\n\n\n\n \nModel\nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\nTT (Sec)\n\n\n\n\net\nExtra Trees Classifier\n0.9854\n0.9991\n0.9937\n0.9775\n0.9855\n0.9708\n0.9710\n2.1500\n\n\nrf\nRandom Forest Classifier\n0.9839\n0.9996\n0.9967\n0.9718\n0.9841\n0.9678\n0.9682\n2.7610\n\n\nlightgbm\nLight Gradient Boosting Machine\n0.9835\n0.9989\n0.9984\n0.9696\n0.9837\n0.9670\n0.9674\n1.7530\n\n\ndt\nDecision Tree Classifier\n0.9798\n0.9798\n0.9978\n0.9632\n0.9802\n0.9596\n0.9603\n0.1410\n\n\ngbc\nGradient Boosting Classifier\n0.9524\n0.9897\n0.9744\n0.9335\n0.9534\n0.9048\n0.9058\n14.4960\n\n\nada\nAda Boost Classifier\n0.8774\n0.9541\n0.8707\n0.8825\n0.8765\n0.7547\n0.7549\n4.4750\n\n\nqda\nQuadratic Discriminant Analysis\n0.8308\n0.9030\n0.8701\n0.8068\n0.8372\n0.6617\n0.6638\n0.1070\n\n\nknn\nK Neighbors Classifier\n0.8267\n0.8966\n0.9460\n0.7641\n0.8453\n0.6535\n0.6730\n0.2130\n\n\nlda\nLinear Discriminant Analysis\n0.7917\n0.8640\n0.7905\n0.7928\n0.7914\n0.5834\n0.5837\n0.1850\n\n\nridge\nRidge Classifier\n0.7906\n0.0000\n0.7812\n0.7965\n0.7886\n0.5812\n0.5816\n0.0470\n\n\nnb\nNaive Bayes\n0.7831\n0.8283\n0.8226\n0.7624\n0.7913\n0.5662\n0.5681\n0.0790\n\n\nlr\nLogistic Regression\n0.7791\n0.8607\n0.7528\n0.7950\n0.7731\n0.5583\n0.5594\n2.4650\n\n\nsvm\nSVM - Linear Kernel\n0.6879\n0.0000\n0.5977\n0.7870\n0.6204\n0.3759\n0.4159\n0.1510\n\n\ndummy\nDummy Classifier\n0.4993\n0.5000\n0.5000\n0.2497\n0.3330\n0.0000\n0.0000\n0.0760\n\n\n\n\n\n\n\n\n\n3.6.1 Light Gradient Boosting Machine\n\nlightgbm = create_model('lightgbm')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9850\n0.9991\n1.0000\n0.9709\n0.9852\n0.9700\n0.9704\n\n\n1\n0.9768\n0.9978\n1.0000\n0.9557\n0.9774\n0.9536\n0.9546\n\n\n2\n0.9795\n0.9997\n1.0000\n0.9607\n0.9800\n0.9591\n0.9599\n\n\n3\n0.9823\n0.9993\n0.9918\n0.9733\n0.9825\n0.9645\n0.9647\n\n\n4\n0.9864\n0.9989\n1.0000\n0.9735\n0.9866\n0.9727\n0.9731\n\n\n5\n0.9877\n0.9997\n0.9973\n0.9786\n0.9878\n0.9754\n0.9756\n\n\n6\n0.9836\n0.9983\n0.9973\n0.9707\n0.9838\n0.9673\n0.9676\n\n\n7\n0.9823\n0.9982\n0.9973\n0.9682\n0.9825\n0.9645\n0.9650\n\n\n8\n0.9877\n0.9985\n1.0000\n0.9760\n0.9879\n0.9754\n0.9757\n\n\n9\n0.9836\n0.9996\n1.0000\n0.9683\n0.9839\n0.9673\n0.9678\n\n\nMean\n0.9835\n0.9989\n0.9984\n0.9696\n0.9837\n0.9670\n0.9674\n\n\nStd\n0.0033\n0.0006\n0.0025\n0.0065\n0.0032\n0.0066\n0.0064\n\n\n\n\n\n\n\n\n\n\n3.6.2 Gradient Boosting Classifier\n\ngbc = create_model('gbc')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9591\n0.9925\n0.9700\n0.9493\n0.9596\n0.9181\n0.9184\n\n\n1\n0.9386\n0.9847\n0.9564\n0.9237\n0.9398\n0.8772\n0.8778\n\n\n2\n0.9495\n0.9906\n0.9782\n0.9253\n0.9510\n0.8990\n0.9005\n\n\n3\n0.9523\n0.9927\n0.9646\n0.9415\n0.9529\n0.9045\n0.9048\n\n\n4\n0.9482\n0.9851\n0.9728\n0.9273\n0.9495\n0.8963\n0.8974\n\n\n5\n0.9563\n0.9932\n0.9699\n0.9441\n0.9569\n0.9127\n0.9130\n\n\n6\n0.9577\n0.9879\n0.9863\n0.9328\n0.9588\n0.9154\n0.9169\n\n\n7\n0.9563\n0.9903\n0.9809\n0.9349\n0.9573\n0.9127\n0.9138\n\n\n8\n0.9536\n0.9917\n0.9727\n0.9368\n0.9544\n0.9072\n0.9079\n\n\n9\n0.9523\n0.9887\n0.9918\n0.9190\n0.9540\n0.9045\n0.9074\n\n\nMean\n0.9524\n0.9897\n0.9744\n0.9335\n0.9534\n0.9048\n0.9058\n\n\nStd\n0.0057\n0.0029\n0.0098\n0.0092\n0.0055\n0.0113\n0.0114\n\n\n\n\n\n\n\n\n\n\n3.6.3 Random Forest\n\nrf = create_model('rf')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9864\n0.9999\n0.9973\n0.9760\n0.9865\n0.9727\n0.9729\n\n\n1\n0.9795\n0.9997\n1.0000\n0.9607\n0.9800\n0.9591\n0.9599\n\n\n2\n0.9850\n0.9997\n0.9973\n0.9734\n0.9852\n0.9700\n0.9703\n\n\n3\n0.9850\n0.9991\n0.9891\n0.9811\n0.9851\n0.9700\n0.9700\n\n\n4\n0.9850\n0.9998\n1.0000\n0.9709\n0.9852\n0.9700\n0.9704\n\n\n5\n0.9891\n0.9999\n1.0000\n0.9786\n0.9892\n0.9782\n0.9784\n\n\n6\n0.9795\n0.9988\n0.9918\n0.9680\n0.9798\n0.9591\n0.9594\n\n\n7\n0.9795\n0.9995\n0.9918\n0.9680\n0.9798\n0.9591\n0.9594\n\n\n8\n0.9877\n0.9999\n1.0000\n0.9760\n0.9879\n0.9754\n0.9757\n\n\n9\n0.9823\n0.9999\n1.0000\n0.9657\n0.9826\n0.9645\n0.9651\n\n\nMean\n0.9839\n0.9996\n0.9967\n0.9718\n0.9841\n0.9678\n0.9682\n\n\nStd\n0.0033\n0.0004\n0.0040\n0.0060\n0.0033\n0.0067\n0.0066\n\n\n\n\n\n\n\n\n\n\n3.6.4 Decision Tree Clasifier\n\ndt = create_model('dt')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.9823\n0.9822\n0.9973\n0.9683\n0.9826\n0.9645\n0.9650\n\n\n1\n0.9782\n0.9781\n0.9973\n0.9606\n0.9786\n0.9563\n0.9570\n\n\n2\n0.9782\n0.9781\n0.9973\n0.9606\n0.9786\n0.9563\n0.9570\n\n\n3\n0.9768\n0.9768\n0.9973\n0.9581\n0.9773\n0.9536\n0.9544\n\n\n4\n0.9809\n0.9809\n1.0000\n0.9633\n0.9813\n0.9618\n0.9625\n\n\n5\n0.9850\n0.9850\n0.9945\n0.9759\n0.9851\n0.9700\n0.9702\n\n\n6\n0.9768\n0.9768\n0.9973\n0.9580\n0.9772\n0.9536\n0.9544\n\n\n7\n0.9823\n0.9823\n0.9973\n0.9682\n0.9825\n0.9645\n0.9650\n\n\n8\n0.9768\n0.9768\n1.0000\n0.9556\n0.9773\n0.9536\n0.9546\n\n\n9\n0.9809\n0.9809\n1.0000\n0.9632\n0.9812\n0.9618\n0.9625\n\n\nMean\n0.9798\n0.9798\n0.9978\n0.9632\n0.9802\n0.9596\n0.9603\n\n\nStd\n0.0027\n0.0027\n0.0016\n0.0058\n0.0026\n0.0054\n0.0052\n\n\n\n\n\n\n\n\n\n\n3.6.5 Ada Boost Classifier\n\nada = create_model('ada')\n\n\n\n\n\n\n\n\n\n \nAccuracy\nAUC\nRecall\nPrec.\nF1\nKappa\nMCC\n\n\nFold\n \n \n \n \n \n \n \n\n\n\n\n0\n0.8936\n0.9591\n0.8937\n0.8937\n0.8937\n0.7872\n0.7872\n\n\n1\n0.8568\n0.9422\n0.8447\n0.8659\n0.8552\n0.7135\n0.7137\n\n\n2\n0.8690\n0.9495\n0.8719\n0.8672\n0.8696\n0.7381\n0.7381\n\n\n3\n0.8677\n0.9522\n0.8665\n0.8689\n0.8677\n0.7353\n0.7353\n\n\n4\n0.8677\n0.9485\n0.8501\n0.8814\n0.8655\n0.7353\n0.7358\n\n\n5\n0.8854\n0.9650\n0.8607\n0.9052\n0.8824\n0.7708\n0.7717\n\n\n6\n0.8745\n0.9486\n0.8661\n0.8806\n0.8733\n0.7490\n0.7491\n\n\n7\n0.8799\n0.9569\n0.8743\n0.8840\n0.8791\n0.7599\n0.7599\n\n\n8\n0.8990\n0.9629\n0.9016\n0.8967\n0.8992\n0.7981\n0.7981\n\n\n9\n0.8799\n0.9565\n0.8770\n0.8819\n0.8795\n0.7599\n0.7599\n\n\nMean\n0.8774\n0.9541\n0.8707\n0.8825\n0.8765\n0.7547\n0.7549\n\n\nStd\n0.0123\n0.0068\n0.0167\n0.0124\n0.0125\n0.0245\n0.0245"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#feature-selection",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#feature-selection",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "3.7 Feature Selection",
    "text": "3.7 Feature Selection\nFeature Selection (pemilihan fitur) adalah proses memilih subset fitur yang paling relevan dan informatif dari suatu dataset untuk digunakan dalam pembangunan model atau analisis data. Tujuan utama dari feature selection adalah untuk meningkatkan kinerja model dengan mengurangi kompleksitas dan meningkatkan interpretabilitas, sambil mempertahankan atau meningkatkan keakuratan prediksi. Pada proses seleksi fitur ini menggunakan fungsi Mutual Information.\nMutual Information (Informasi Timbal Balik) adalah suatu metrik yang digunakan untuk mengukur seberapa banyak pengetahuan tentang suatu variabel dapat memberikan wawasan tentang variabel lainnya. Dalam konteks feature selection atau pemilihan fitur, mutual information digunakan untuk mengukur seberapa informatif suatu fitur terhadap variabel target atau variabel kelas.\nRumus Mutual Information :\n\\[I(X;Y) = \\sum_{x \\in X} \\sum_{y \\in Y} p(x, y) \\log\\left(\\frac{p(x, y)}{p(x)p(y)}\\right)\\]\ndimana :\n- \\(p(x,y)\\) adalah probabilitas bersama dari \\(X\\) dan \\(Y\\). - \\(p(x)\\) dan \\(p(y)\\) adalah probabilitas marginal dari \\(X\\) dan \\(Y\\) masing - masing.\nInformation gain diukur sebagai perbedaan antara entropi set data awal dan entropi set data setelah pemisahan berdasarkan fitur A. Semakin tinggi information gain, semakin baik fitur A dalam memisahkan data.\n1) Berikut merupakan langkah-langkah dalam menghitung information gain :\n\nHitung entropy awal (sebelum pemisahan):\nHitung entropy dataset sebelum dilakukan pemisahan. Berikut merupakan formulanya :\n\\[ H(S) = -\\sum_{i=1}^{n} p_i \\cdot \\log_2(p_i) \\]\nKeterangan :\nH(S) = nilai entropy awal n = jumlah kelas Pi = proporsi kelas i dalam dataset\nHitung entropy setelah pemisahan: Menghitung entropy tiap kelompok yang dihasilkan setelah pemisahan berdasarkan suatu fitur. Berikut merupakan formulanya :\n\\[ H(S|A) = \\sum_{j=1}^{v} \\frac{|S_j|}{|S|} \\cdot H(S_j) \\]\nKeterangan : H(S|A) = entropy dataset setelah pemisahan pada fitur A v = jumlah nilai dalam fitur A | Sj | = ukuran subset dari fitur A yang mempunyai nilai j | S | = ukuran dataset H(Sj) = entropy subset untuk fitur A\nHitung information gain: \\[ \\text{Gain}(S, A) = H(S) - H(S|A) \\]\nKeterangan : H(S) = entropy awal dataset S H(S|A) = entropy dataset setelah pemisahan pada fitur A\n\n2) Berikut merupakan contoh perhitungan manual menggunakan data dummy :\n\n\n\nID\nOutlook\nTemperature\nPlay Tennis\n\n\n\n\n1\nSunny\nHot\nNo\n\n\n2\nSunny\nHot\nNo\n\n\n3\nOvercast\nHot\nYes\n\n\n4\nRainy\nMild\nYes\n\n\n5\nRainy\nCool\nYes\n\n\n6\nRainy\nCool\nNo\n\n\n7\nOvercast\nCool\nYes\n\n\n8\nSunny\nMild\nNo\n\n\n9\nSunny\nCool\nYes\n\n\n10\nRainy\nMild\nYes\n\n\n\n\nMenghitung entropy awal :\n\\[ H(S) = -p_{\\text{Yes}} \\cdot \\log_2(p_{\\text{Yes}}) - p_{\\text{No}} \\cdot \\log_2(p_{\\text{No}}) \\]\n\\[ H(S) = -[\\frac{6}{10} \\cdot \\log_2\\left(\\frac{6}{10}\\right) + \\frac{4}{10} \\cdot \\log_2\\left(\\frac{4}{10}\\right) ] \\]\n\\[ H(S) = -\\left(\\frac{6}{10} \\log_2\\left(\\frac{3}{5}\\right) + \\frac{4}{10} \\log_2\\left(\\frac{2}{5}\\right)\\right) \\]\n\\[ H(S) = -\\left(-0.4422 - 0.5288\\right) \\]\n\\[ H(S) = 0.971 \\]\nMenghitung entropy setelah pemisahan (berdasarkan outlook) :\n(a) Subset SUNNY \\[  [ H(S_{\\text{Sunny}}) = -\\left(\\frac{3}{5} \\cdot \\log_2\\left(\\frac{3}{5}\\right) + \\frac{2}{5} \\cdot \\log_2\\left(\\frac{2}{5}\\right)\\right) ] \\]\n\\[ H(S_{\\text{Sunny}}) = -\\left(\\frac{3}{5} \\cdot (-0.737) + \\frac{2}{5} \\cdot (-1.322)\\right) \\]\n\\[ [ H(S_{\\text{Sunny}}) = 0.971 ] \\]\n(b) Subset OVERCAST \\[ [ H(S_{\\text{Overcast}}) = 0 ] (karena  semua instance positif) \\]\n(c) Subset RAINY \\[ [ H(S_{\\text{Rainy}}) = -\\left(\\frac{2}{5} \\cdot \\log_2\\left(\\frac{2}{5}\\right) + \\frac{3}{5} \\cdot \\log_2\\left(\\frac{3}{5}\\right)\\right) ] \\]\n\\[ [ H(S_{\\text{Rainy}}) = -\\left(\\frac{2}{5} \\cdot (-0.737) + \\frac{3}{5} \\cdot (-0.971)\\right) ] \\]\n\\[ [ H(S_{\\text{Rainy}}) = 0.971 ] \\]\n(d) Entropy setelah pemisahan \\[  [ H(S|\\text{Outlook}) = \\frac{5}{10} \\cdot H(S_{\\text{Sunny}}) + \\frac{4}{10} \\cdot H(S_{\\text{Overcast}}) + \\frac{5}{10} \\cdot H(S_{\\text{Rainy}}) ] \\]\n\\[ H(S_{\\text{Overcast}}) + \\frac{5}{10} \\cdot H(S_{\\text{Rainy}}) \\] \\[ H(S|\\text{Outlook}) = \\frac{5}{10} \\cdot 0.971 + \\frac{4}{10} \\cdot 0 + \\frac{5}{10} \\cdot 0.971 \\]\n\\[ [ H(S|\\text{Outlook}) = 0.971 ] \\]\n\n\ndf = pd.read_csv('data_normalisasi.csv')\nX = df.drop('is_safe', axis=1)\ny = df['is_safe']\n# Menghitung Information Gain untuk setiap fitur\ninformation_gains = mutual_info_classif(X, y)\n\n# Menyusun hasil ke dalam DataFrame untuk analisis\nfeature_scores = pd.DataFrame({'feature': X.columns, 'information_gain': information_gains})\nfeature_scores = feature_scores.sort_values(by='information_gain', ascending=False)\n\n# Menampilkan hasil Information Gain untuk setiap fitur\nprint(feature_scores)\n\n# Plotting bar plot untuk perbandingan Information Gain\nplt.figure(figsize=(12, 6))\nplt.barh(feature_scores['feature'], feature_scores['information_gain'], color='lightgreen')\nplt.xlabel('Information Gain')\nplt.ylabel('Feature')\nplt.title('Features by Information Gain')\nplt.show()\n\n        feature  information_gain\n15  perchlorate          0.331623\n1       ammonia          0.301489\n12     nitrates          0.222200\n0     aluminium          0.216562\n4       cadmium          0.193228\n5    chloramine          0.180673\n16       radium          0.142545\n2       arsenic          0.119687\n3        barium          0.092841\n13     nitrites          0.087851\n6      chromium          0.063050\n7        copper          0.038504\n11         lead          0.036719\n10      viruses          0.035334\n8      flouride          0.022959\n18       silver          0.019000\n9      bacteria          0.018768\n19      uranium          0.011103\n14      mercury          0.003603\n17     selenium          0.000720\n\n\n\n\n\n\n3.7.1 Pencarian Fitur Terbaik Light Gradient Boosting Machine\n\nbest_accuracy = 0\nbest_k = 0\nbest_feature_scores = None\nbest_selected_feature_names = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Light Gradient Boosting Machine\n    lightgbm_model = lightgbm\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    lightgbm_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_lightgbm = lightgbm_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Light Gradient Boosting Machine\n    accuracy_lightgbm = accuracy_score(y_test, y_pred_lightgbm)\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_lightgbm &gt; best_accuracy:\n        best_accuracy = accuracy_lightgbm\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        model = lightgbm_model\n        scaler = k_best_selector\n\n        # Menyimpan skor korelasi untuk fitur terpilih\n        best_feature_scores = k_best_selector.scores_\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Light Gradient Boosting Machine :\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fiturLightGradientBoostingMachine.pkl')\n\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001877 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3034\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001930 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2834\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 19\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.002033 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2683\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 18\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001672 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2582\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 17\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000441 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2571\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 16\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001447 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2316\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 15\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001375 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 2115\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 14\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000343 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 2104\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 13\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001715 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 1860\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 12\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000400 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1605\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 11\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000259 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1353\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 10\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000245 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 1098\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 9\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000861 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 843\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 8\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000558 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 833\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 7\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000615 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 782\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 6\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000352 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 721\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 5\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000491 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 630\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 4\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000232 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 524\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 3\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n\n\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.000081 seconds.\nYou can set `force_row_wise=true` to remove the overhead.\nAnd if memory is not enough, you can set `force_col_wise=true`.\n[LightGBM] [Info] Total Bins 269\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 2\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.000137 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 23\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 1\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n[LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n\n\nJumlah Fitur : 20\nBest Fitur Light Gradient Boosting Machine : Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nflouride: 2.422481294450292\nbacteria: 4.429129304026364\nviruses: 173.48651984813026\nlead: 0.020259579232659978\nnitrates: 87.74346761883795\nnitrites: 78.22425346978895\nmercury: 33.07324163404674\nperchlorate: 94.43379608807072\nradium: 92.15581382719945\nselenium: 14.847178481026724\nsilver: 161.19923638886357\nuranium: 153.99965019028522\n\n\n['fiturLightGradientBoostingMachine.pkl']\n\n\n\n# Membaca DataFrame\ndf = pd.read_csv('data_balancing.csv')\n\n# Memuat list fitur yang akan di-drop dari file pickle\nLightGradientBoostingMachine_drop = joblib.load('fiturLightGradientBoostingMachine.pkl')\n\n# Menghilangkan 'is_safe' dari setiap nama fitur\nLightGradientBoostingMachine_drop = [feature.replace('is_safe', '') for feature in LightGradientBoostingMachine_drop]\n\n# Memilih hanya kolom yang ada di pickle\ndata_LightGradientBoostingMachine_drop = df[LightGradientBoostingMachine_drop + ['is_safe']]\n\n# Menampilkan DataFrame setelah fitur di-drop\ndata_LightGradientBoostingMachine_drop.head()\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n3.7.2 Pencarian Fitur Terbaik Gradient Boosting Classifier\n\nbest_accuracy = 0\nbest_k = 0\nbest_selected_feature_names = None\nbest_feature_scores = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Gradient Boosting\n    gb_model = gbc\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    gb_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_gb = gb_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Gradient Boosting\n    accuracy_gb = accuracy_score(y_test, y_pred_gb)\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_gb &gt; best_accuracy:\n        best_accuracy = accuracy_gb\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        model = gb_model\n        scaler = k_best_selector\n\n        # Menyimpan skor korelasi untuk fitur terpilih\n        best_feature_scores = k_best_selector.scores_\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Gradient Boosting :\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fiturgradientboosting.pkl')\n\nJumlah Fitur : 20\nBest Fitur Gradient Boosting : Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nflouride: 2.422481294450292\nbacteria: 4.429129304026364\nviruses: 173.48651984813026\nlead: 0.020259579232659978\nnitrates: 87.74346761883795\nnitrites: 78.22425346978895\nmercury: 33.07324163404674\nperchlorate: 94.43379608807072\nradium: 92.15581382719945\nselenium: 14.847178481026724\nsilver: 161.19923638886357\nuranium: 153.99965019028522\n\n\n['fiturgradientboosting.pkl']\n\n\n\n# Membaca DataFrame\ndf = pd.read_csv('data_balancing.csv')\n\n# Memuat list fitur yang akan di-drop dari file pickle\nGradientBoostingClasifier_drop = joblib.load('fiturgradientboosting.pkl')\n\n# Menghilangkan 'is_safe' dari setiap nama fitur\nGradientBoostingClasifier_drop = [feature.replace('is_safe', '') for feature in GradientBoostingClasifier_drop]\n\n# Memilih hanya kolom yang ada di pickle\ndata_GradientBoostingClasifier_drop = df[GradientBoostingClasifier_drop + ['is_safe']]\n\n# Menampilkan DataFrame setelah fitur di-drop\ndata_GradientBoostingClasifier_drop.head()\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n3.7.3 Pencarian Fitur Terbaik Random Forest\n\nbest_accuracy = 0\nbest_k = 0\nbest_selected_feature_names = None\nbest_feature_scores = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Random Forest\n    rf_model = rf\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    rf_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_rf = rf_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Random Forest\n    accuracy_rf = accuracy_score(y_test, y_pred_rf)\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_rf &gt; best_accuracy:\n        best_accuracy = accuracy_rf\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        model = rf_model\n        scaler = k_best_selector\n\n        # Menyimpan skor korelasi untuk fitur terpilih\n        best_feature_scores = k_best_selector.scores_\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Random Forest:\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fiturrandomforest.pkl')\n\nJumlah Fitur : 18\nBest Fitur Random Forest: Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'bacteria', 'viruses', 'nitrates', 'nitrites',\n       'mercury', 'perchlorate', 'radium', 'selenium', 'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nbacteria: 2.422481294450292\nviruses: 4.429129304026364\nnitrates: 173.48651984813026\nnitrites: 0.020259579232659978\nmercury: 87.74346761883795\nperchlorate: 78.22425346978895\nradium: 33.07324163404674\nselenium: 94.43379608807072\nsilver: 92.15581382719945\nuranium: 14.847178481026724\n\n\n['fiturrandomforest.pkl']\n\n\n\n# Membaca DataFrame\ndf = pd.read_csv('data_balancing.csv')\n\n# Memuat list fitur yang akan di-drop dari file pickle\nRandomForest_drop = joblib.load('fiturrandomforest.pkl')\n\n# Menghilangkan 'is_safe' dari setiap nama fitur\nRandomForest_drop = [feature.replace('is_safe', '') for feature in RandomForest_drop]\n\n# Memilih hanya kolom yang ada di pickle\ndata_RandomForest_drop = df[RandomForest_drop + ['is_safe']]\n\n# Menampilkan DataFrame setelah fitur di-drop\ndata_RandomForest_drop.head()\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nbacteria\nviruses\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.20\n0.000\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.65\n0.650\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.05\n0.003\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n0.71\n0.710\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.13\n0.001\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n\n\n\n\n\n\n\n3.7.4 Decision Tree Clasifier\n\nbest_accuracy = 0\nbest_k = 0\nbest_selected_feature_names = None\nbest_feature_scores = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Decision Tree\n    dt_model = DecisionTreeClassifier()  # Ganti dengan parameter yang sesuai\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    dt_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_dt = dt_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Decision Tree\n    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_dt &gt; best_accuracy:\n        best_accuracy = accuracy_dt\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        model = dt_model\n        scaler = k_best_selector\n\n        # Menyimpan skor korelasi untuk fitur terpilih\n        best_feature_scores = k_best_selector.scores_\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Decision Tree:\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fiturdecisiontree.pkl')\n\nJumlah Fitur : 20\nBest Fitur Decision Tree: Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nflouride: 2.422481294450292\nbacteria: 4.429129304026364\nviruses: 173.48651984813026\nlead: 0.020259579232659978\nnitrates: 87.74346761883795\nnitrites: 78.22425346978895\nmercury: 33.07324163404674\nperchlorate: 94.43379608807072\nradium: 92.15581382719945\nselenium: 14.847178481026724\nsilver: 161.19923638886357\nuranium: 153.99965019028522\n\n\n['fiturdecisiontree.pkl']\n\n\n\nbest_accuracy = 0\nbest_k = 0\nbest_selected_feature_names = None\nbest_feature_scores = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Random Forest\n    dt_model = dt\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    dt_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_dt = dt_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Random Forest\n    accuracy_dt = accuracy_score(y_test, y_pred_dt)\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_dt &gt; best_accuracy:\n        best_accuracy = accuracy_dt\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        model = dt_model\n        scaler = k_best_selector\n\n        # Menyimpan skor korelasi untuk fitur terpilih\n        best_feature_scores = k_best_selector.scores_\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Decision Tree:\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fiturdecisiontree.pkl')\n\nJumlah Fitur : 20\nBest Fitur Decision Tree: Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nflouride: 2.422481294450292\nbacteria: 4.429129304026364\nviruses: 173.48651984813026\nlead: 0.020259579232659978\nnitrates: 87.74346761883795\nnitrites: 78.22425346978895\nmercury: 33.07324163404674\nperchlorate: 94.43379608807072\nradium: 92.15581382719945\nselenium: 14.847178481026724\nsilver: 161.19923638886357\nuranium: 153.99965019028522\n\n\n['fiturdecisiontree.pkl']\n\n\n\n# Membaca DataFrame\ndf = pd.read_csv('data_balancing.csv')\n\n# Memuat list fitur yang akan di-drop dari file pickle\nDecisionTree_drop = joblib.load('fiturdecisiontree.pkl')\n\n# Menghilangkan 'is_safe' dari setiap nama fitur\nDecisionTree_drop = [feature.replace('is_safe', '') for feature in DecisionTree_drop]\n\n# Memilih hanya kolom yang ada di pickle\ndata_DecisionTree_drop = df[DecisionTree_drop + ['is_safe']]\n\n# Menampilkan DataFrame setelah fitur di-drop\ndata_DecisionTree_drop.head()\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n\n\n5 rows × 21 columns\n\n\n\n\n\n3.7.5 Ada Boost Classifier\n\nfrom sklearn.feature_selection import SelectKBest, f_classif\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nimport joblib\n\nbest_accuracy = 0\nbest_k = 0\nbest_selected_feature_names = None\nbest_feature_scores = None\nmodel = None\nscaler = None\n\nfor k in range(X_train.shape[1], 0, -1):\n    # Menghitung nilai korelasi antara setiap fitur dengan target\n    k_best_selector = SelectKBest(f_classif, k=k)\n    X_train_selected = k_best_selector.fit_transform(X_train, y_train)\n    X_test_selected = k_best_selector.transform(X_test)\n\n    # Mendapatkan nama fitur terpilih\n    selected_feature_names = X_ros.columns[k_best_selector.get_support(indices=True)]\n\n    # Membuat model Adaboost\n    adaboost_model = AdaBoostClassifier(n_estimators=50, random_state=42)  # Ganti dengan parameter yang sesuai\n\n    # Melatih model menggunakan data training yang telah dinormalisasi dan terpilih fiturnya\n    adaboost_model.fit(X_train_selected, y_train)\n\n    # Melakukan prediksi pada data testing yang telah dinormalisasi dan terpilih fiturnya\n    y_pred_adaboost = adaboost_model.predict(X_test_selected)\n\n    # Mengukur akurasi model Adaboost\n    accuracy_adaboost = accuracy_score(y_test, y_pred_adaboost)\n\n    # Menyimpan skor korelasi untuk fitur terpilih\n    feature_scores = k_best_selector.scores_\n\n    # Membandingkan dengan akurasi terbaik sejauh ini\n    if accuracy_adaboost &gt; best_accuracy:\n        best_accuracy = accuracy_adaboost\n        best_k = k\n        best_selected_feature_names = selected_feature_names\n        best_feature_scores = feature_scores\n        model = adaboost_model\n        scaler = k_best_selector\n\n# Menampilkan hasil terbaik\nprint(\"Jumlah Fitur :\", best_k)\nprint(\"Best Fitur Adaboost:\", best_selected_feature_names)\nprint(\"\\nBest Fitur Scores:\")\nfor feature_name, score in zip(best_selected_feature_names, best_feature_scores):\n    print(f\"{feature_name}: {score}\")\n\n# Menyimpan nama fitur terpilih\njoblib.dump(best_selected_feature_names, 'fituradaboost.pkl')\n\nJumlah Fitur : 20\nBest Fitur Adaboost: Index(['aluminium', 'ammonia', 'arsenic', 'barium', 'cadmium', 'chloramine',\n       'chromium', 'copper', 'flouride', 'bacteria', 'viruses', 'lead',\n       'nitrates', 'nitrites', 'mercury', 'perchlorate', 'radium', 'selenium',\n       'silver', 'uranium'],\n      dtype='object')\n\nBest Fitur Scores:\naluminium: 1768.3190428266103\nammonia: 24.082352474449795\narsenic: 609.9877500155244\nbarium: 122.3111242384997\ncadmium: 1961.2468311627367\nchloramine: 705.6421255995247\nchromium: 609.1375894392714\ncopper: 28.20858760070382\nflouride: 2.422481294450292\nbacteria: 4.429129304026364\nviruses: 173.48651984813026\nlead: 0.020259579232659978\nnitrates: 87.74346761883795\nnitrites: 78.22425346978895\nmercury: 33.07324163404674\nperchlorate: 94.43379608807072\nradium: 92.15581382719945\nselenium: 14.847178481026724\nsilver: 161.19923638886357\nuranium: 153.99965019028522\n\n\n['fituradaboost.pkl']\n\n\n\n# Membaca DataFrame\ndf = pd.read_csv('data_balancing.csv')\n\n# Memuat list fitur yang akan di-drop dari file pickle\nAdaboost_drop = joblib.load('fituradaboost.pkl')\n\n# Menghilangkan 'is_safe' dari setiap nama fitur\nAdaboost_drop = [feature.replace('is_safe', '') for feature in Adaboost_drop]\n\n# Memilih hanya kolom yang ada di pickle\ndata_Adaboost_drop = df[Adaboost_drop + ['is_safe']]\n\n# Menampilkan DataFrame setelah fitur di-drop\ndata_Adaboost_drop.head()\n\n\n\n\n\n\n\n\naluminium\nammonia\narsenic\nbarium\ncadmium\nchloramine\nchromium\ncopper\nflouride\nbacteria\n...\nlead\nnitrates\nnitrites\nmercury\nperchlorate\nradium\nselenium\nsilver\nuranium\nis_safe\n\n\n\n\n0\n1.65\n9.08\n0.04\n2.85\n0.007\n0.35\n0.83\n0.17\n0.05\n0.20\n...\n0.054\n16.08\n1.13\n0.007\n37.75\n6.78\n0.08\n0.34\n0.02\n1\n\n\n1\n2.32\n21.16\n0.01\n3.31\n0.002\n5.28\n0.68\n0.66\n0.90\n0.65\n...\n0.100\n2.01\n1.93\n0.003\n32.26\n3.21\n0.08\n0.27\n0.05\n1\n\n\n2\n1.01\n14.02\n0.04\n0.58\n0.008\n4.24\n0.53\n0.02\n0.99\n0.05\n...\n0.078\n14.16\n1.11\n0.006\n50.28\n7.07\n0.07\n0.44\n0.01\n0\n\n\n3\n1.36\n11.33\n0.04\n2.96\n0.001\n7.23\n0.03\n1.66\n1.08\n0.71\n...\n0.016\n1.41\n1.29\n0.004\n9.12\n1.72\n0.02\n0.45\n0.05\n1\n\n\n4\n0.92\n24.33\n0.03\n0.20\n0.006\n2.67\n0.69\n0.57\n0.61\n0.13\n...\n0.117\n6.74\n1.11\n0.003\n16.90\n2.41\n0.02\n0.06\n0.02\n1\n\n\n\n\n5 rows × 21 columns"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#light-gradient-boosting-machine-1",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#light-gradient-boosting-machine-1",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.1 Light Gradient Boosting Machine",
    "text": "4.1 Light Gradient Boosting Machine\nMetode Light Gradient Boosting Machine (LightGBM) merupakan salah satu algoritma machine learning yang termasuk dalam kategori ensemble learning, khususnya dalam keluarga algoritma gradient boosting. LightGBM dikembangkan oleh Microsoft dan dirancang untuk memberikan kinerja yang cepat dan efisien, terutama pada dataset yang besar.\nBerikut adalah beberapa karakteristik utama dari LightGBM:\n\nGradient Boosting Framework, jadi LightGBM merupakan implementasi dari algoritma gradient boosting, yang berfokus pada membangun model prediktif dengan cara menggabungkan beberapa model lemah (weak learners) menjadi satu model yang kuat. Proses ini dilakukan secara iteratif, dengan setiap iterasi bertujuan untuk memperbaiki kesalahan prediksi yang dibuat oleh model sebelumnya.\nLeaf-wise Growth Strategy, Salah satu fitur utama dari LightGBM adalah strategi pertumbuhan pohon secara daun (leaf-wise). Berbeda dengan pendekatan level-wise pada algoritma gradient boosting lainnya, LightGBM membangun pohon dengan mengekspansi daun yang memberikan kontribusi terbesar pada penurunan kesalahan (loss). Strategi ini membantu meningkatkan efisiensi karena hanya daun yang signifikan yang diperluas.\nHistogram-Based Learning, LightGBM menggunakan histogram untuk memproses data pada setiap iterasi pembelajaran. Histogram adalah representasi diskrit dari distribusi data, yang membantu mengurangi waktu komputasi. Proses pembelajaran pada LightGBM membagi dataset ke dalam beberapa bucket histogram, dan kemudian menghitung statistik di setiap bucket untuk mempercepat pembelajaran.\nCategorical Feature Support, LightGBM memiliki dukungan khusus untuk fitur kategorikal tanpa perlu melakukan pre-processing one-hot encoding. Ini membuatnya lebih efisien untuk menangani dataset dengan fitur kategorikal.\nDistributed Training, LightGBM mendukung pelatihan terdistribusi, yang memungkinkan pengguna melatih model pada dataset yang sangat besar dengan menggunakan beberapa sumber daya komputasi secara bersamaan.\nRegularization,LightGBM menyediakan opsi untuk menerapkan regularisasi pada pohon untuk mencegah overfitting. Regularisasi dapat diatur menggunakan parameter seperti lambda dan alpha.\nKecepatan dan Skalabilitas, Salah satu keunggulan utama LightGBM adalah kecepatan eksekusinya yang tinggi dan skalabilitasnya terhadap ukuran dataset. Hal ini membuatnya menjadi pilihan populer untuk tugas-tugas yang melibatkan data besar.\n\n\nfrom sklearn.preprocessing import RobustScaler\n\nX = data_LightGradientBoostingMachine_drop.drop('is_safe', axis=1)\ny = data_LightGradientBoostingMachine_drop['is_safe']\nX_train_lgbm, X_test_lgbm, y_train_lgbm, y_test_lgbm = train_test_split(X, y, test_size=0.2, random_state=42)\n\nRobustScaler = RobustScaler()\nX_train_lightgbm = RobustScaler.fit_transform(X_train_lgbm)\nX_test_lightgbm = RobustScaler.transform(X_test_lgbm)\n\nlightgbm_model = lightgbm\nlightgbm_model.fit(X_train_lightgbm, y_train_lgbm)\ny_pred_lightgbm = lightgbm_model.predict(X_test_lightgbm)\naccuracy_lightgbm = accuracy_score(y_test_lgbm, y_pred_lightgbm)\nprint(\"\\nAccuracy metode Light Gradient Boosting Machine :\",accuracy_lightgbm*100)\n\njoblib.dump(RobustScaler, 'saclelightgbm.pkl')\njoblib.dump(lightgbm_model, 'modellightgbm.pkl')\n\n[LightGBM] [Info] Number of positive: 4180, number of negative: 4197\n[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003947 seconds.\nYou can set `force_col_wise=true` to remove the overhead.\n[LightGBM] [Info] Total Bins 3018\n[LightGBM] [Info] Number of data points in the train set: 8377, number of used features: 20\n[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.498985 -&gt; initscore=-0.004059\n[LightGBM] [Info] Start training from score -0.004059\n\nAccuracy metode Light Gradient Boosting Machine : 98.37708830548925\n\n\n['modellightgbm.pkl']"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#gradient-boosting-classifier-1",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#gradient-boosting-classifier-1",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.2 Gradient Boosting Classifier",
    "text": "4.2 Gradient Boosting Classifier\nMetode Gradient Boosting adalah salah satu pendekatan dalam machine learning yang termasuk dalam kategori ensemble learning. Ensemble learning melibatkan penggabungan beberapa model lemah (weak learners) untuk membentuk model yang lebih kuat. Gradient Boosting secara khusus fokus pada pengurangan kesalahan prediksi dengan memperhatikan kesalahan model sebelumnya.\nBerikut adalah konsep dasar dari metode Gradient Boosting:\n\nWeak learners adalah model prediktif yang memiliki performa yang sedikit di atas random chance. Contohnya termasuk decision trees dengan kedalaman yang sangat terbatas (stump) atau regresi linear sederhana. Dalam konteks Gradient Boosting, sejumlah besar weak learners akan digabungkan untuk membentuk model yang kuat. Sequential Model Building:\nProses Gradient Boosting melibatkan pembangunan model secara iteratif. Pada setiap iterasi, model baru ditambahkan untuk mengkompensasi kesalahan prediksi yang belum ditangani oleh model sebelumnya. Model yang baru ditambahkan harus “mengajar” untuk kesalahan yang ada. Ini dilakukan dengan meminimalkan residual error (selisih antara prediksi aktual dan prediksi model sebelumnya) menggunakan weak learner.\nLoss function (fungsi kerugian) digunakan untuk mengukur sejauh mana model saat ini memberikan prediksi yang benar atau mendekati target aktual. Pada setiap iterasi, model berusaha meminimalkan nilai loss function. Contoh loss function untuk regresi adalah Mean Squared Error (MSE), sementara untuk klasifikasi bisa menggunakan log-likelihood atau Gini Index.\nGradient descent digunakan untuk menemukan parameter model yang meminimalkan loss function. Pada setiap iterasi, model baru ditambahkan dengan menyesuaikan parameter (misalnya, bobot pohon pada decision tree) dalam arah yang berlawanan dengan gradien loss function. Proses ini memungkinkan model untuk fokus pada bagian dataset yang sulit diprediksi oleh model sebelumnya.\nShrinkage adalah parameter yang mengontrol seberapa besar kita akan “mempelajari” dari setiap model tambahan. Nilai shrinkage yang lebih rendah memerlukan lebih banyak iterasi untuk mencapai kinerja yang baik, tetapi dapat meningkatkan kestabilan model.\nBeberapa implementasi Gradient Boosting, seperti XGBoost atau LightGBM, memiliki opsi untuk menerapkan teknik regularisasi untuk mencegah overfitting. Regularisasi dapat diatur dengan menggunakan parameter tertentu, seperti lambda dan alpha.\nGradient Boosting mirip dengan metode AdaBoost, tetapi ada perbedaan kunci. Jika AdaBoost memberikan “perhatian lebih” pada contoh yang salah diprediksi sebelumnya, Gradient Boosting fokus pada residual errors atau gradien loss function.\n\n\nfrom sklearn.preprocessing import RobustScaler\n\nX = data_GradientBoostingClasifier_drop.drop('is_safe', axis=1)\ny = data_GradientBoostingClasifier_drop['is_safe']\nX_train_gbc, X_test_gbc, y_train_gbc, y_test_gbc = train_test_split(X, y, test_size=0.2, random_state=42)\n\nRobustScaler = RobustScaler()\nX_train_GradientBoosting = RobustScaler.fit_transform(X_train_gbc)\nX_test_GradientBoosting = RobustScaler.transform(X_test_gbc)\n\ngbc_model = gbc\ngbc_model.fit(X_train_GradientBoosting, y_train_gbc)\ny_pred_gbc = gbc_model.predict(X_test_GradientBoosting)\naccuracy_gbc = accuracy_score(y_test_gbc, y_pred_gbc)\nprint(\"Accuracy metode Gradient Boosting Clasifier :\",accuracy_gbc*100)\n\njoblib.dump(RobustScaler, 'saclergbc.pkl')\njoblib.dump(gbc_model, 'modelgbc.pkl')\n\nAccuracy metode Gradient Boosting Clasifier : 95.13126491646779\n\n\n['modelgbc.pkl']"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#random-forest-1",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#random-forest-1",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.3 Random Forest",
    "text": "4.3 Random Forest\nRandom Forest adalah sebuah metode ensemble learning yang digunakan dalam pemodelan prediktif dan klasifikasi. Ensemble learning melibatkan penggabungan hasil dari beberapa model untuk meningkatkan kinerja dan akurasi keseluruhan. Random Forest khususnya menggunakan pohon keputusan sebagai model dasar dan menggabungkan prediksi dari beberapa pohon keputusan untuk membuat keputusan akhir.\nRandom Forest efektif untuk berbagai jenis tugas seperti klasifikasi dan regresi, dan sering digunakan karena kemampuannya yang baik dalam menangani data yang kompleks dan beragam. Keunggulan utamanya termasuk kemampuan untuk mengatasi overfitting, memberikan nilai penting untuk fitur, dan memberikan performa yang baik secara umum.\nBerikut adalah contoh sederhana bagaimana kita bisa menghitung prediksi menggunakan Random Forest dengan tiga pohon:\nMisalkan kita memiliki tiga pohon keputusan yang masing-masing memberikan prediksi sebagai berikut untuk suatu titik data:\n\n\n\nPohon\nPrediksi\n\n\n\n\n1\nA\n\n\n2\nB\n\n\n3\nA\n\n\n\nDalam hal ini, Random Forest akan memberikan prediksi akhir “Kelas A” karena ini adalah kelas yang paling sering diprediksi oleh pohon-pohon dalam hutan.\n\nContoh Penerapan\n\nMisalkan kita memiliki data kategori obesitas dengan fitur seperti Usia, Tinggi (TB), dan Berat Badan (BB), dan target kita adalah kategori obesitas. Kita bisa menggunakan Random Forest untuk memprediksi jumlah unit yang akan diproduksi berdasarkan fitur-fitur tersebut.\n1. Persiapan Data\n\n\n\nUsia\nTinggi Badan\nBerat Badan\nTarget Kategori\n\n\n\n\n25\n170\n70\nNormal\n\n\n30\n170\n60\nNormal\n\n\n35\n180\n80\nObesitas\n\n\n\n2. Pelatihan Model\nKedua, kita melatih model Random Forest menggunakan data historis tersebut. Model akan mempelajari hubungan kompleks antara “Usia”, “Tinggi Badan”, “Berat Badan”, dan target “Kategori Obesitas”.\n3. Penggunaan Model Untuk Prediksi\nSetelah melatih model, kita dapat menggunakannya untuk memprediksi kategori obesitas berdasarkan fitur-fitur baru. Misalkan kita memiliki data baru seperti berikut:\n\n\n\nUsia\nTinggi Badan\nBerat Badan\n\n\n\n\n28\n175\n75\n\n\n\n4. Hasil Prediksi Model kita kemudian memberikan prediksi kategori obesitas berikut:\n\n\n\nUsia\nTinggi Badan\nBerat Badan\nTarget Kategori\n\n\n\n\n28\n175\n75\nObesitas\n\n\n\nDalam contoh ini, model Random Forest memprediksi bahwa dengan usia 28 tahun, tinggi badan 175 cm, dan berat badan 75 kg, seseorang termasuk dalam kategori “Obesitas”.\n\nfrom sklearn.preprocessing import RobustScaler\n\nX = data_RandomForest_drop.drop('is_safe', axis=1)\ny = data_RandomForest_drop['is_safe']\nX_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X, y, test_size=0.2, random_state=42)\n\nRobustScaler = RobustScaler()\nX_train_RandomForest = RobustScaler.fit_transform(X_train_rf)\nX_test_RandomForest = RobustScaler.transform(X_test_rf)\n\nrf_model = rf\nrf_model.fit(X_train_RandomForest, y_train_rf)\ny_pred_rf = rf_model.predict(X_test_RandomForest)\naccuracy_rf = accuracy_score(y_test_rf, y_pred_rf)\nprint(\"Accuracy metode Random Forest :\",accuracy_rf*100)\n\njoblib.dump(RobustScaler, 'saclerrandomforest.pkl')\njoblib.dump(rf_model, 'modelrandomforest.pkl')\n\nAccuracy metode Random Forest : 98.71121718377088\n\n\n['modelrandomforest.pkl']"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#decision-tree-clasifier-2",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#decision-tree-clasifier-2",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.4 Decision Tree Clasifier",
    "text": "4.4 Decision Tree Clasifier\nDecision Tree, atau pohon keputusan, adalah salah satu algoritma pembelajaran mesin yang dapat digunakan untuk tugas klasifikasi dan regresi. Algoritma ini membangun struktur pohon di mana setiap simpul (node) mewakili keputusan berdasarkan fitur-fitur data. Pada setiap simpul, algoritma membuat keputusan berdasarkan fitur tertentu, sehingga memungkinkan pemisahan data menjadi kelompok yang semakin homogen.\nKeunggulan Decision Tree: 1. Interpretabilitas: Pohon keputusan dapat dengan mudah diinterpretasikan oleh manusia karena strukturnya mirip dengan keputusan yang diambil berdasarkan aturan sederhana.\n\nPenanganan Fitur Non-Lineer: Decision Tree dapat menangani hubungan non-linear antara fitur dan target.\nSkalabilitas: Cocok untuk dataset dengan banyak fitur dan data yang cukup besar.\n\n\nContoh Penerapan:\n\n\nPersiapan Data Misalkan kita memiliki data mengenai konsumen yang ingin kita kelompokkan berdasarkan keputusan pembelian suatu produk. Data tersebut mungkin terlihat seperti ini:\n\n\n\n\nUmur\nPendapatan\nPendidikan\nKeputusan Pembelian\n\n\n\n\n25\n50000\nSarjana\nYa\n\n\n35\n75000\nMaster\nTidak\n\n\n45\n100000\nDoktor\nYa\n\n\n\n\nPelatihan Model Selanjutnya, kita melatih model Decision Tree menggunakan data historis tersebut. Model akan mempelajari aturan-aturan keputusan berdasarkan fitur-fitur seperti “Umur”, “Pendapatan”, dan “Pendidikan”.\nPenggunaan Model Untuk Prediksi Setelah pelatihan, kita dapat menggunakan model untuk memprediksi keputusan pembelian konsumen baru. Misalkan kita memiliki data baru:\n\n\n\n\nUmur\nPendapatan\nPendidikan\n\n\n\n\n30\n60000\nDiploma\n\n\n\n\nHasil Prediksi\n\nModel Decision Tree kemudian memberikan prediksi keputusan pembelian, misalnya, “Ya” atau “Tidak”, berdasarkan aturan yang telah dipelajari dari data historis.\n\nfrom sklearn.preprocessing import RobustScaler\n\nX = data_DecisionTree_drop.drop('is_safe', axis=1)\ny = data_DecisionTree_drop['is_safe']\nX_train_dt, X_test_dt, y_train_dt, y_test_dt = train_test_split(X, y, test_size=0.2, random_state=42)\n\nRobustScaler = RobustScaler()\nX_train_DecisionTree = RobustScaler.fit_transform(X_train_dt)\nX_test_DecisionTree = RobustScaler.transform(X_test_dt)\n\ndt_model = dt\ndt_model.fit(X_train_DecisionTree, y_train_dt)\ny_pred_dt = dt_model.predict(X_test_DecisionTree)\naccuracy_dt = accuracy_score(y_test_dt, y_pred_dt)\nprint(\"Accuracy metode Decision Tree :\",accuracy_dt*100)\n\njoblib.dump(RobustScaler, 'saclerdecisiontree.pkl')\njoblib.dump(dt_model, 'modeldecisiontree.pkl')\n\nAccuracy metode Decision Tree : 98.18615751789976\n\n\n['modeldecisiontree.pkl']"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#ada-boost-classifier-2",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#ada-boost-classifier-2",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.5 Ada Boost Classifier",
    "text": "4.5 Ada Boost Classifier\nAdaboost menggabungkan hasil dari sejumlah besar weak classifiers (misalnya, pohon keputusan yang sangat sederhana) untuk membentuk model yang kuat. Setiap weak classifier diberi bobot berdasarkan seberapa baik mereka menangani sampel pada iterasi sebelumnya. Proses ini dilakukan secara iteratif, dan pada setiap iterasi, bobot baru diberikan pada sampel yang salah diprediksi pada iterasi sebelumnya.\nDengan kata lain, jika kita memiliki M weak classifiers, prediksi model ensemble Adaboost (( F(x) )) dapat dihitung dengan:\n\\[  F(x) = \\sum_{m=1}^{M} \\alpha_m \\cdot f_m(x)  \\]\ndi mana: - $ F(x) $ adalah prediksi model ensemble. - $ _m $ adalah bobot yang diberikan pada weak classifier ke-m. - $ f_m(x) $ adalah prediksi dari weak classifier ke-m.\nBobot $ _m $ dihitung berdasarkan tingkat kesalahan weak classifier pada iterasi sebelumnya. Semakin baik weak classifier menangani sampel, semakin besar bobotnya.\nBentuk umum ini mencerminkan kombinasi linear dari weak classifiers, dan dengan strategi ini, Adaboost dapat memperbaiki performa model terhadap sampel yang sulit diprediksi oleh model sebelumnya.\n\nfrom sklearn.preprocessing import RobustScaler\n\nX = data_Adaboost_drop.drop('is_safe', axis=1)\ny = data_Adaboost_drop['is_safe']\nX_train_ada, X_test_ada, y_train_ada, y_test_ada = train_test_split(X, y, test_size=0.2, random_state=42)\n\nRobustScaler = RobustScaler()\nX_train_adaboost = RobustScaler.fit_transform(X_train_ada)\nX_test_adaboost = RobustScaler.transform(X_test_ada)\n\nada_model = ada\nada_model.fit(X_train_adaboost, y_train_ada)\ny_pred_ada = ada_model.predict(X_test_adaboost)\naccuracy_ada = accuracy_score(y_test_ada, y_pred_ada)\nprint(\"Accuracy metode Adaboost :\",accuracy_ada*100)\n\njoblib.dump(RobustScaler, 'sacleradaboost.pkl')\njoblib.dump(ada_model, 'modeladaboost.pkl')\n\nAccuracy metode Adaboost : 87.9236276849642\n\n\n['modeladaboost.pkl']"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#perbandingan-metode",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#perbandingan-metode",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "4.6 Perbandingan Metode",
    "text": "4.6 Perbandingan Metode\n\nmethods = ['Light Gradient Boosting Machine', 'Gradient Boosting', 'Random Forest', 'Decision Tree', 'Adaboost']\n\n# Daftar akurasi dari setiap metode\naccuracies = [accuracy_lightgbm*100, accuracy_gbc*100, accuracy_rf*100, accuracy_dt*100, accuracy_ada*100]\n\n# Membuat dataframe dari data\ndata = {'Metode': methods, 'Akurasi (%)': accuracies}\ndataa = pd.DataFrame(data)\n\n# Membuat grafik bar dengan Seaborn\nplt.figure(figsize=(18, 8))\nax = sns.barplot(x='Metode', y='Akurasi (%)', data=dataa, palette='pastel')\nplt.xlabel('Metode')\nplt.ylabel('Akurasi (%)')\nplt.title('Perbandingan Akurasi antara Metode')\nplt.ylim(0, 100)\n# Menambahkan nilai-nilai akurasi di atas setiap batang\nfor index, value in enumerate(accuracies):\n    plt.text(index, value - 50, f'{value:.2f}%', ha='center', fontsize=12)\n\nplt.show()"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#light-gradient-boosting-machine-2",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#light-gradient-boosting-machine-2",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "5.1 Light Gradient Boosting Machine",
    "text": "5.1 Light Gradient Boosting Machine\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_lgbm, y_pred_lightgbm))\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_lgbm, y_pred_lightgbm)\nclass_labels = np.unique(np.concatenate((y_test_lgbm, y_pred_lightgbm)))\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.98      1039\n           1       0.97      1.00      0.98      1056\n\n    accuracy                           0.98      2095\n   macro avg       0.98      0.98      0.98      2095\nweighted avg       0.98      0.98      0.98      2095"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#gradient-boosting-classifier-2",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#gradient-boosting-classifier-2",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "5.2 Gradient Boosting Classifier",
    "text": "5.2 Gradient Boosting Classifier\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_gbc, y_pred_gbc))\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_gbc, y_pred_gbc)\nclass_labels = np.unique(np.concatenate((y_test_gbc, y_pred_gbc)))\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.97      0.93      0.95      1039\n           1       0.93      0.97      0.95      1056\n\n    accuracy                           0.95      2095\n   macro avg       0.95      0.95      0.95      2095\nweighted avg       0.95      0.95      0.95      2095"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#random-forest-2",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#random-forest-2",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "5.3 Random Forest",
    "text": "5.3 Random Forest\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_rf, y_pred_rf))\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_rf, y_pred_rf)\nclass_labels = np.unique(np.concatenate((y_test_rf, y_pred_rf)))\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.99      1039\n           1       0.98      1.00      0.99      1056\n\n    accuracy                           0.99      2095\n   macro avg       0.99      0.99      0.99      2095\nweighted avg       0.99      0.99      0.99      2095"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#decision-tree-clasifier-3",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#decision-tree-clasifier-3",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "5.4 Decision Tree Clasifier",
    "text": "5.4 Decision Tree Clasifier\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_dt, y_pred_dt))\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_dt, y_pred_dt)\nclass_labels = np.unique(np.concatenate((y_test_dt, y_pred_dt)))\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       1.00      0.97      0.98      1039\n           1       0.97      1.00      0.98      1056\n\n    accuracy                           0.98      2095\n   macro avg       0.98      0.98      0.98      2095\nweighted avg       0.98      0.98      0.98      2095"
  },
  {
    "objectID": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#ada-boost-classifier-3",
    "href": "PSD_Mubessirul Ummah_210411100140_Water Quality.html#ada-boost-classifier-3",
    "title": "1  BUSINESS UNDERSTANDING",
    "section": "5.5 Ada Boost Classifier",
    "text": "5.5 Ada Boost Classifier\n\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_test_ada, y_pred_ada))\nfrom sklearn.metrics import confusion_matrix\n\ncm = confusion_matrix(y_test_ada, y_pred_ada)\nclass_labels = np.unique(np.concatenate((y_test_ada, y_pred_ada)))\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", xticklabels=class_labels, yticklabels=class_labels)\nplt.xlabel('Predicted')\nplt.ylabel('Actual')\nplt.title('Confusion Matrix')\nplt.show()\n\n              precision    recall  f1-score   support\n\n           0       0.87      0.89      0.88      1039\n           1       0.89      0.87      0.88      1056\n\n    accuracy                           0.88      2095\n   macro avg       0.88      0.88      0.88      2095\nweighted avg       0.88      0.88      0.88      2095"
  },
  {
    "objectID": "summary.html",
    "href": "summary.html",
    "title": "2  Summary",
    "section": "",
    "text": "In summary, this book has no content whatsoever."
  }
]